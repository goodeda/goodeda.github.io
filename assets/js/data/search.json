[ { "title": "Vaikea suomi", "url": "/posts/vaikeasuomi-p%C3%A4iv%C3%A4kirja/", "categories": "Suomi(Finnish)", "tags": "päiväkirja", "date": "2022-10-12 23:00:00 +0300", "snippet": "Opiskelin Essiivi ja Translatiivi kesäyliopiston kurssilla. Tunnen itseni väsyneeksi on oikea, muuta en voi käyttää väsyneenä. Toinen esimerki on: Pidän heitä ystävällisinä on oikea. Ajattelen, että jos asia kestää pitkää aikaa (olomuoto), sitten käyttää Essiivi. Jos se on lyhyt ja vain on muutos, käyttää Translatiivi. Usein tunnen sekavalta siistä. Miksi suomella on tosi paljon sääntöä…" }, { "title": "Hyvää syksyä", "url": "/posts/syksy-p%C3%A4iv%C3%A4kirja/", "categories": "Suomi(Finnish)", "tags": "päiväkirja", "date": "2022-10-09 17:00:00 +0300", "snippet": "Kaunis taivas! Tyykkän tästä ilmastosta. Tietän syksy on ihan lyhyt Suomessa. Mahdollisesti sata lunta kuukauden jälkeen, siksi vaalitaan ja muistetaan tätä aikaa. Niin, tänään minä päätin matkustamaan ulkomailla 21-25. Olen ostanut lentolippun ja varanut huonetta.Kultainen metsä" }, { "title": "Minun tutkielma", "url": "/posts/tutkielma-p%C3%A4iv%C3%A4kirja/", "categories": "Suomi(Finnish)", "tags": "päiväkirja", "date": "2022-10-07 15:00:00 +0300", "snippet": "Hyvää viestiä! Minulla on valvoja minun tutkielmasta. Hän on tohtori kandidaatti osastossani. Tutkielmani aihe on Vähän-resursseja käyttävää kielin käännös ja minusta valvojani on kokenut. Uskon, että hän osaa auttaa minua paljon. Seuravaa päämäärä minulle on kirjoitamaan tutkimuksen suunnitelman." }, { "title": "Elokuva--Avatar", "url": "/posts/elokuva-p%C3%A4iv%C3%A4kirja/", "categories": "Suomi(Finnish)", "tags": "päiväkirja", "date": "2022-10-06 20:00:00 +0300", "snippet": "Menin elokuvateatteriin ystävin kanssa ja katsoimme Avatar! Se on klassikko fiktio elokuva, joka vapautetaan 2009 vuonna. Ajattelen tämä elokuva on hyvä, vaan se on myös tosi pitkä. Minä istuin melkein kolme tuntia! Uutiset sanovat, että Avatar 2 saa ensi-iltansa Suomessa neljäs Joulukuussa tänä vuonna. Kuulosta hyvältä!" }, { "title": "Keskustelu ystäväni kanssa", "url": "/posts/keskustelu-p%C3%A4iv%C3%A4kirja/", "categories": "Suomi(Finnish)", "tags": "päiväkirja", "date": "2022-09-30 23:00:00 +0300", "snippet": "Tänään tutustuin minun ystävään kurssin kuulutua. Me kävelin kaupungin keskuksesta Pasilaan ja keskustelimme elämästämme. Minä kanelin, että on tosi vaikea löydä työpaikka Suomessa kansavälisille opiskelijoille. Vaikka opiskelen tietojenkasittelytietetta, luulen paljon työpaikkaa tarvitse suomen kielen taitoa. Muuta auringonlasku on ihan kaunis. Toivottavasti huomenna on parempi." }, { "title": "App analysis with review information on google play", "url": "/posts/app-visual/", "categories": "English", "tags": "Visualization,, POWER-BI", "date": "2022-09-14 23:13:00 +0300", "snippet": "IntroductionEach international student coming to finland has to consider opening a bank account for strong authentication which is a necessity in Finland. The access to identify yourself online greatly facilitate the administration. Besides, you need a bank account to make consumptions, transfer, get salaries or make a loan. There are three major banks that people use everyday.I used python api and scraped reviews of those three applications. By visualising the data, I make it easier for people to have a quick look at the reputation of these three banks.ReportAnalysis/InterpretationThe data is automatically collected with google-play-scrape API and it’s as of the beginning of September so might not be the latest values shown on google play store. I only analyze the results I got and won’t make any personal judgement on the apps.Overall scoreNordea has the most highest average (4.16) score among the three banks. The rating of Danske bank is the lowest, 2.61/5.0.Relation between Review length and scoreLooking at the plot on upper right side, there is an interesting trend of review length and score. If the score goes higher, the review tends to be shorter. Otherwise, people complain more and the ratings drop. This is particulary true in Nordea. We can see a nosedive in score over the time of June, 2018. The plot actually makes sense because if customers are satisfyied with the app, they usually just comment with “hyva” “helppo” or “good”. However, if the app broke down or some bugs happened, users are more likely to complain a lot about the problems.Something must happened in 2018 as all apps witnessed a dramastic drop on the ratings. It could be caused by version update or some regulations.Review compositionFrom the doughnut chart, we can see Finnish users are dominant in three apps. English is the second largest part and swedish follows as the third.Customer service replyI used an area plot to show the reply rates of different banks. Although Danske is rated as the lowest, its customer service is making the best to reply each comments. The two almost fully overlapping areas indicates the reply rate is very high.In terms of Nordea, because of a larger number of comments, comparing with the other two, it replies not so frequently. The large gap exists also because most comments are positive (rating is high) and there is probably no need to make a response. OP bank’s performance is the average level.App timeBased on the collected data, we also know the operating time of Nordea and OP banks is longer, from 2014 to now, while the danske bank entered into application market since 2017. To some extent, I also believe the duration of the app influences its performance because if a app exists longer, it has more time to adjust and update versions to improve its usability.ConclusionThere are actually more information can be explored. For example, what did customer complain about in reviews? Does frequent iteration of versions affect the app reputation? Sentiment analysis is feasible but it could be challenging to do so as there are just a few models in dealing with Finnish texts.I feel quite proud of this mini-report because I do it from scratch, from data collection, data wrangling to final visualization. There must be many places can be improved and I wish to develop my visualization skills more in the future projects or work." }, { "title": "Logistic Regression", "url": "/posts/Logistic-Regression/", "categories": "English", "tags": "math, machine-learning", "date": "2022-09-09 15:23:00 +0300", "snippet": "As I’m taking the Models and algorithms in NLP-application course, here I take notes of some classical machine learning models. In addition, I try to build the model without importing sklearn packages.A brief backgroundI believe linear regression is a widely-known quantitative methods in many fields especially in economics. It gives weights to variables and then add a bias as a random noise. The the formula:\\(\\hat{y} = a_1*x_1 + a_2*x_2 + a_3*x_3 +b\\)written in matrix: $Y = A*X + b$ measures a linear relationship between variables and target.However, this method can’t really deal with classification task because the range of $\\hat{y}$ can be infinite. So, we try to limit the value in an interval of 0 and 1, representing the probability of the label. Then, the Sigmoid function is introduced. It is formulated as $y = \\frac{1}{1+e^{-x}}$ . The range of y is between 0 and 1, which perfectly meets the requirement. In another word, a linear regression plus the sigmoid function is the logistic regressThen next part discusses how to apply this approach in binary-classification task.Define a loss functionSo far we have defined the function to fit data. There is a question that how we measure if the function fits data well or not? Loss function is what we need. Before defining the loss function, we should consider another thing.The probability of label should have something to do with the predicted result $\\hat{y}$ and the true result $y$.Here:\\(\\begin{equation}P = \\hat{y}^{y}*(1-\\hat{y})^{1-y}\\end{equation}\\)When y =1, the probability is $\\hat{y}$, and when y = 0, the probability equals to $1-\\hat{y}$. This means \\(\\hat{y}=p\\{y=1|x, a\\}\\), given data x and weight a.If we log on both sides, then it turns to:\\[\\log P = y*\\log \\hat{y} + (1-y)*\\log (1-\\hat{y})\\]if y = 1, $\\hat{y}$ should be close to 1, otherwise loss goes upif y = 0, $\\hat{y}$ should be close to 0, otherwise loss goes up.However, in the above formula, when y =1, $\\log P = \\hat{y}$ is negative infinite. We hope it could be positive infinite. Therefore, the loss function is:\\[\\begin{equation} L(\\hat{y}, y) = -(y*\\log \\hat{y} + (1-y)*\\log (1-\\hat{y}))\\end{equation}\\]Calculate gradientsNow there is a loss function, then we need to update the weights. So we need to calculate the gradients about A and b.\\[\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial (AX+b)} * \\frac{\\partial (AX+b)}{\\partial A} \\\\\\]The first and last items are pretty easy to compute. Let’s look at the Sigmoid function.\\[\\begin{equation} \\begin{aligned}Sigmoid(x)^{&#39;}&amp;amp;=(\\frac{1}{1+e^{-x}})^{&#39;}\\\\&amp;amp;= [(1+e^{-x})^{-1}]^{&#39;} \\\\&amp;amp;= -1* (1+e^{-x})^{-2}*(-e^{-x}) \\\\&amp;amp;= \\frac{1}{1+e^{-x}}*\\frac{e^{-x}}{1+e^{-x}} \\\\&amp;amp; = sig * (1- sig)\\end{aligned}\\end{equation}\\]Thus, the equation can be concluded as:\\[\\begin{equation} \\begin{aligned}\\frac{\\partial L}{\\partial A} &amp;amp;= -y*\\frac{1}{\\hat{y}}*\\hat{y} (1-\\hat{y})*X + (1-y)*\\frac{1}{1-\\hat{y}}(\\hat{y}(1-\\hat{y}))*X \\\\&amp;amp;= y*(\\hat{y}-1)*X + (1-y)*\\hat{y}*X \\\\&amp;amp;= (\\hat{y}-y)*X \\\\\\end{aligned}\\end{equation}\\]The same for the bias gradient\\[\\begin{equation}\\begin{aligned}\\frac{\\partial L}{\\partial b} &amp;amp;= \\frac{\\partial L}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial (AX+b)} * \\frac{\\partial (AX+b)}{\\partial b} \\\\&amp;amp;= \\hat{y}-y\\end{aligned}\\end{equation}\\]Then the weights will be updated through: $A = A - \\alpha* \\frac{\\partial L}{\\partial A}$More analysisThe original linear model is very useful in regression and classification (with sigmoid). There is also a concern about the formula due to the lack of weight limitation. We notice that the there is no limition on the values of weights. That’s to say, the weights could be extremely large or small. There is a risk of overfitting and unfortunately, this will lead to very volatile results. –Suppose the data value is always large in training dataset, but the model doesn’t work well when it meets small values.One solution is to add a penalty on the loss function. The penalty can be $\\Sigma a_{i}^{2}$ or $\\Sigma|a_{i}|$, former of which is called Ridge Regression and the latter one is Lasso Regression. The benefit of doing so is the control of $A$ so that the values would not be too large. Besides, Lasso method is actually “selecting” useful features because it will make the parameter close to 0 as much as possible. For those variables whose weights are 0, they contribute nothing to the result and to some extent, only valuable variables will be kept. In fact, the penalty can be applied in classification to reduce the variation.CodeComing soon." }, { "title": "Virtual machine on Windows", "url": "/posts/bisystem/", "categories": "English", "tags": "Linux", "date": "2022-09-02 14:22:00 +0300", "snippet": "As I program more, I find linux system is more efficient for my work. However, windows is still a good option in terms of everyday use. Thereby I come up an idea that why not have two systems on one machine, and I can use linux for working and windows for other use. Here is how I configurated it. I take a note on the process in case of any need again in the future.Installing Virtual machineTo have a virtual OS on the laptop, we have to install a virtual machine software where we can emulate the system.THe installation instruction can be found hereUse VirtualBox mainly because it’s free and light! Plus, it’s very friendly to new users.Setting up system In VirtualBox, go for setting, find Shared Folder menubar and click it. On the right side, add new shared folder and select the folder path, remember to check the box of auto-mount. You can also name the shared folder. Then remember to set your root password. When you log in the Ubuntu, for example, you only need to set up the username as well as its password. However, this is not your root password. In order to get the admin access, enter su root passwd in the terminal. Afterwards, enter you first type your own user password and then type your new root password. It’s time to make the shared directory. sudo mkdir &amp;lt;the shared folder path you want to build&amp;gt;sudo mount -t vboxsf &amp;lt;name of the shared folder(which you named in step 1)&amp;gt; &amp;lt;shared folder path set above&amp;gt;OK, now if you use ls to check the current files and folders, you’ll find the shared folder with a green background box. Unfortunately, it always requires the mount command every time you reopen the machine. I didn’t figure out how to make it auto mount under the root setting.But I set an alias in .bashrc file so that user can just type a short command to repeat it. add the command alias &amp;lt;a command name you like&amp;gt;=&#39;sudo mount -t vboxsf &amp;lt;name of the shared folder(which you named in step 1)&amp;gt; &amp;lt;shared folder path set above&amp;gt;&#39;. Now you’re ready to go! We can edit the files on both systems and they are shared!Push to Github on linux Just follow the SSH blog. Git clone your repo. The terminal may pop up some , asking who are you. Then you need togit config --global user.name &amp;lt;YOUR_NAME&amp;gt;git config --global user.email &quot;&amp;lt;YOUR_EMAIL&amp;gt; Great! Now we can add or modify files of the repository! With an external monitor, we even can operate two systems on the same machine. I feel it looks pretty cool and boosts my productivity!Some useful links:https://askubuntu.com/questions/161759/how-to-access-a-shared-folder-in-virtualboxhttps://linuxconfig.org/how-to-set-a-root-password-on-ubuntu-22-04-jammy-jellyfish-linux" }, { "title": "Power BI practice for data visualization", "url": "/posts/Power-BI-Practice/", "categories": "English", "tags": "Visualization,, POWER-BI", "date": "2022-08-30 00:29:00 +0300", "snippet": "Here I’m excited to show some outputs of my weekend study.The following interface is based on the data from power bi udemy courseThat’s really cool, isn’t it? I’m planning to do another project with real-life data. Probably update it some time later!" }, { "title": "Password-less for connecting Github and remote server", "url": "/posts/SSH-KEY/", "categories": "English", "tags": "Linux", "date": "2022-08-24 22:30:00 +0300", "snippet": "I used to enter the password when pushing file to Github or connecting to remote server. I always feel this step redundant and recently came to konw that SSH is a good solution. I want to record these steps in case of need later.SSH for Github Running the following command:ssh-keygen -t rsa&amp;lt;keep clicking &#39;Enter&#39; if system asks you for actions&amp;gt;You can also add -C &quot;your_email@example.com&quot; at the end of ssh-keygen -t rsa to make a copy of the ssh-key by email. Check your ~/.ssh folder and there is a id_rsa.pub Add the key to github account. Change to the git repo folder and run: git remote set-url origin git@github.com:username/your-repository.git, here the origin could also be master. Then try git push and this time it shouldn’t require you to enter password anymore.Remote Server If you already have a key, then run ssh-copy-id &amp;lt;your server address&amp;gt; If you don’t have, you can just follow the step 1&amp;amp;2 for github and do the above step. You can directly log in when next time you try to connect server with ssh &amp;lt;server&amp;gt;.More details please see FOR GITHUB and SSH FOR UBUNTU." }, { "title": "Elämäni Suomessa", "url": "/posts/Yksi-vuosi-suomessa/", "categories": "Suomi(Finnish)", "tags": "Language-learning", "date": "2022-08-21 07:22:00 +0300", "snippet": "Aikani SuomessaTulin Suomeen viime vuonna tänään. Vielä muistin, että kantoin kaksi matkalaukkua ja kävin kaupunkikeskuksessa. Se oli ensimainen kertani käytämään P-junaa. Olen asunnut Helsingissä yksi vuosi. Minä pitään Helsingistä, vaikka se ei ole suuria. Ihmiset täällä ovät ystävällinen.OpiskelemisestaHelsingin yliopisto on kuuluisa maailmassa. Voin tutustua kansainvälisiin koulukavereihin yliopistossa. Minä nautin yliopiston opiskelutyylistä koska osaan valita kursseja vapaasti. Kirjasto on minun lempipaikkani, jossa on viileä ja rauhallinen koko vuoden. Voin lainata erilaisia kirjoja tuosta. Osallistun usein ohjelmani keskusteluun ja toimimisiin. Kurssit myös kiinnostavat minua.ElämästäOlen tottunut Suomen kylmään ilmastoon, vaikka tulen Kiinan eteläosasta, jonka lämpötila usein on kaskikymmentä astetta. Olen lähellä luontoa koska voin kävellä metsässä, uida meressä tai grillata järvellä. Ruoka täällä Suomessa on yksinkertainen, vaan joskus menen kiinalaisiin ravintoloihin syömään. Oikeastaan tunnen myös, että helppo ruoka on terveellinen. Minun täytty oppia mitä teen ruoka kotona siksi ruoanlaittoni edistyy paljon.Euro käytetään Suomessa. Paljon tapahtumaa järjestetään Kaupungissa, esimerkiksi elokuva-ja taidefestivaali. Minäkin matkustan ulkomaissa Euroopassa. Kävin Ruotsissa, Tanskassa, Saksassa ja Tšekissa. Matkat ovat tosi helppo sen takia, että olen EU-asukas ja en tarvitse turistiviisumia. Kauppassa ostaminen on mukava koska osaat aina löytää K-market, S-market, Lidl tai Prisma. Julkinen liikenne on myös kehittynyt hyvin ja opiskelijoilla on alennus, joka on melkein viisikymmentä prosenttia. Syöminen on sama. Tämä alennus todella säästää rahaani. Elämä yleisesti on ihana paitsi pitkiä ja pimeitä talvia.ToiveeniToivon palatamaan Kiinaan seuraavana vuonna koska en ole tavattu perheenjäseniäni pitkään. Lisäksi haluaisin työskentellä Euroopassa insinööriksi tulevaisuudessa. Samaan aikaan suunnittelen YKI testiä koska haluan tarkistaa suomen kielen tasoani. Minäkin jatkan suomen oppimista ja toivottavasti osaan puhua suomea sujuvasti.Kiitos lukemistasi!" }, { "title": "Analyse the loss of airline client", "url": "/posts/Prediction-of-client-loss/", "categories": "English, Projects", "tags": "Projects", "date": "2022-08-19 09:00:00 +0300", "snippet": "Project descriptionSo this project was acutally done in my undergraduate course–machine learning &amp;amp; data mining. The topic is about the airline company’s customer analysis and prediction. The dataset consists of 60k+ pieces of feature information. I mainly practised using pandas, sklearn and other python packages. In order to run the code, pandas, numpy, seaborn, matplotlib, scikit-learn and scipy are required.import pandas as pdimport numpy as npimport seaborn as snsimport timeimport matplotlib.pyplot as pltfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split,GridSearchCVfrom sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,accuracy_score,roc_curve,aucfrom sklearn.preprocessing import StandardScalerSeveral packages should be imported before we start the formal work.Know your customers (Data exploration)Reading filedata=pd.read_csv(&quot;airlin_runoff.csv&quot;,encoding=&quot;gb18030&quot;)#make sure the data file in under the same folder where the code file is storedair_data=data.copy()air_data=air_data.iloc[:,2:]#the first two variables are meaningless so just drop itair_data[&quot;runoff_flag&quot;]=air_data[&quot;runoff_flag&quot;].astype(int)The first step–loading the data is done.client featuresplt.bar(air_data[&quot;age&quot;].value_counts().index,air_data[&quot;age&quot;].value_counts(),color=&quot;skyblue&quot;) plt.title(&quot;Distribution of client ages&quot;)plt.xlabel(&quot;Age&quot;)plt.ylabel(&quot;Number of people&quot;)plt.show()AgeBy running above code, we get the distribution graph.From the distribution, we see the major customer group is the middle-aged clients whose age ranges from 30 to 50. However, there are some exceptional data as well. For example, some cases are clients under 18 or above 80. Considering the later prediction task, these cases are very difficult to predict since they are not likely to make decision independently.Filtering missing valuesApart form that, by running air_data.isna().sum(), we can see if there is any missing value in each feature.air_data=air_data[(air_data.age &amp;gt;18) &amp;amp; (air_data.age&amp;lt;80)]air_data=air_data[(air_data.EXPENSE_SUM_YR_1.notna())&amp;amp;(air_data.EXPENSE_SUM_YR_2.notna())&amp;amp;(air_data.age.notna())] The above code only picks up clients who are between 18 and 80 years old and some of their features shouldn’t be missing. This makes our analysis more sensible.print(air_data[&quot;EXPENSE_SUM_YR_1&quot;].value_counts())print(air_data[&quot;EXPENSE_SUM_YR_2&quot;].value_counts())After that we want to check to what are values of EXPENSE_SUM_YR_1/2 which means the expense in year 1 &amp;amp; 2 (let’s suppose it’s last year and the year before last). The results show as follows:air_data=air_data[(air_data.EXPENSE_SUM_YR_1!=0)|(air_data.EXPENSE_SUM_YR_2!=0)] # if the case&#39;s expense in two consecutive years is zero, then drop it.Genderprint(air_data[&quot;GENDER&quot;].value_counts()) # 1 for male and 0 for femaleplt.pie(air_data[&quot;GENDER&quot;].value_counts(),labels=[&quot;M&quot;,&quot;F&quot;],autopct=&#39;%1.1f%%&#39;,colors=sns.color_palette())plt.title(&quot;Male &amp;amp; Female customers&quot;)#plt.savefig(&quot;men&amp;amp;women.png&quot;,dpi=400)plt.show()Almost third fourths of customers are male. Then what can we do for more useful information?print(air_data.groupby([&quot;GENDER&quot;])[&quot;runoff_flag&quot;].value_counts().loc[:,[0,1]])xlabel=[&quot;F&quot;,&quot;M&quot;]y_0=air_data.groupby([&quot;GENDER&quot;])[&quot;runoff_flag&quot;].value_counts().loc[:,0]y_1=air_data.groupby([&quot;GENDER&quot;])[&quot;runoff_flag&quot;].value_counts().loc[:,1]bar_width=0.3plt.bar(range(len(xlabel)),y_0,label=&quot;stayed&quot;,width=bar_width,color=&quot;blue&quot;)plt.bar(np.arange(len(xlabel))+bar_width,y_1,label=&quot;lost&quot;,width=bar_width,color=&quot;red&quot;)plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]plt.xticks(range(len(xlabel)),xlabel)plt.ylabel(&quot;Number of customers&quot;)plt.title(&quot;Stayed and lost clients in gender group&quot;)plt.legend()plt.show()#if there is a significant difference for male and female client lossfrom scipy.stats import chi2_contingencytable = [[8115,6340],[30032,17022]]chi2,pval,dof,expected = chi2_contingency(table)print(&quot;Null hypothesis：There is no significant difference between gender group&quot;)if pval &amp;lt; 0.05: print(&quot;Refuse null hypothesis(choose backup), there is a signigicant difference&quot;)else: print(&quot;Accept null, there is no difference&quot;)As the result shows that there is a certain correlation between gender and clients losts.Membership level# Convert the class into string formatdef classify(x): if x==4.0: return &quot;4-class&quot; elif x==5.0: return &quot;5-class&quot; else: return &quot;6-class&quot;air_data[&quot;FFP_TIER&quot;]=air_data[&quot;FFP_TIER&quot;].apply(classify)print(air_data[&quot;FFP_TIER&quot;].value_counts())# drawing bar chart for the number of customers at different membership levelsplt.bar(air_data[&quot;FFP_TIER&quot;].value_counts().index,air_data[&quot;FFP_TIER&quot;].value_counts(),color=sns.color_palette()[:3])plt.xticks([0,1,2],air_data[&quot;FFP_TIER&quot;].value_counts().index)plt.xlabel(&quot;FFP_TIER&quot;)plt.ylabel(&quot;Number of customers&quot;)plt.title(&quot;Membership level and number&quot;)plt.show()Most of memberships are class-4, so I guess this is a general, basic level.Then let’s see if the level affects customer loss.xlabel=[&quot;4-class&quot;,&quot;5-class&quot;,&quot;6-class&quot;]y_0=[33937,3236,1346]y_1=[24129,173,167]bar_width=0.3plt.bar(range(len(xlabel)),y_0,label=&quot;stayed&quot;,width=bar_width,color=&quot;blue&quot;)plt.bar(np.arange(len(xlabel))+bar_width,y_1,label=&quot;lost&quot;,width=bar_width,color=&quot;red&quot;)plt.xticks([0,1,2],xlabel)plt.ylabel(&quot;Number&quot;)plt.title(&quot;Customer lost at different membership levels&quot;)plt.legend()plt.show()Here from the bar chart, it’s clear that the higher class customers are more loyal. Most losses happen in the class-4 group. Thus, when it comes to taking actions to recall lost customers or keep retention, attention should be put on class-4 group. What’s more, if we want to prevent the situation from happening later, we should attract more customers to update to class-5/6 and raise their loyalty.Membership timeprint(air_data.groupby(&quot;runoff_flag&quot;)[&quot;FFP_days&quot;].describe())plt.boxplot([data.FFP_days[data.runoff_flag==0],data.FFP_days[data.runoff_flag==1]],labels=[&quot;stayed&quot;,&quot;lost&quot;])plt.title(&quot;Stayed and lost customers as well as membership time&quot;)plt.show()FlightsMaybe this plot doesn’t make so much sense in axis-x. It shows how much the area of blue (stayed group) exceeds the red area. Of course, it makes sense that the flights of stayed group are larger.Last flight time gapprint(air_data[&quot;DAYS_FROM_LAST_TO_END&quot;].value_counts())plt.bar(air_data[&quot;DAYS_FROM_LAST_TO_END&quot;].value_counts().index,air_data[&quot;DAYS_FROM_LAST_TO_END&quot;].value_counts(),color=&quot;blue&quot;)plt.title(&quot;The time of last flight to the end of observation&quot;)plt.xlabel(&quot;days&quot;)plt.ylabel(&quot;Number of people&quot;)plt.show()The plot measures the time from the last flight to the end of observation time. For example, those cases whose gap is 400+ days mean the last time they choose the airline is about 1+ years ago.Othersprint(air_data.groupby(&quot;runoff_flag&quot;)[&quot;WEIGHTED_SEG_KM&quot;].describe())print(air_data.groupby(&quot;runoff_flag&quot;)[&quot;avg_discount&quot;].describe())Prediction with machine learningSo far we’ve done pretty much about knowing the customers. This section we should do the prediction task.air_data_ = air_data.iloc[:,:53]air_data_[&quot;FFP_days&quot;]=air_data[&quot;FFP_days&quot;]air_data_ = air_data_.join(pd.get_dummies(air_data.FFP_TIER))#独热编码del air_data_[&quot;FFP_TIER&quot;]air_data_[&quot;label&quot;]=air_data[&quot;runoff_flag&quot;]air_data_.head()Since the membership level is string format with three different types, they should be converted into one-hot encoding. As a result of that, class-4 would be [1, 0, 0] and class-5 should be [0, 1, 0] and class-6 is [0, 0, 1].X=air_data_.iloc[:,:-1]y=air_data_.iloc[:,-1]scaler=StandardScaler()X_=scaler.fit_transform(X)X_train,X_test,y_train,y_test=train_test_split(X_,y,train_size=0.75,random_state=123)Use train_test_split to divide the original data and get ready for model input.Linear Regressionlog_reg = LogisticRegression(max_iter=1000)start=time.perf_counter()log_reg.fit(X_train,y_train)end=time.perf_counter()print(&quot;LogisticRgression model&quot;)print(&quot;Training costs:&quot;,end-start,&quot;s&quot;)log_reg_y_pre=log_reg.predict(X_test)print(&quot;LR accuracy：&quot;,accuracy_score(y_test,log_reg_y_pre))#===================================#log_reg_y_predict=log_reg.predict_proba(X_test)print(&quot;AUC score：&quot;,roc_auc_score(y_test,log_reg_y_predict[:,1]))print(&quot;Confusion matrix\\n&quot;,confusion_matrix(y_test,log_reg_y_pre))The above block returns a very high accuracy–0.99 on test dataset. Nevertheless, the extremely high accuracy is not necessarily a good sign because it could be overfitting. Overfitting happens when the model focuses too much on the provided data and it may lost the generalization when encountering unseen data or some noises.Random ForestRandom forest is a kind of ensemble learning. There is less chance for this model to overfit on this dataset. The model is also usually more robust.But there are actually many key hyperparameters to set up. In order to gain the optimal one, we use GridSearchCV.param_grid = [{&#39;n_estimators&#39;: [20,25,30,35,40,45,50], &#39;max_depth&#39;: [5,6,7,8,9,10]}]rnd_reg = RandomForestClassifier(random_state=123)grid_search = GridSearchCV(rnd_reg, param_grid, cv=5,scoring=&#39;roc_auc&#39;)grid_search.fit(X_train,y_train)print(&quot;Best parameter combination:&quot;,grid_search.best_params_)print(&quot;Best score:&quot;,grid_search.best_score_)The best parameter combination is max_depth as 10 and n_estimator as 45.rnd=RandomForestClassifier(n_estimators=45,max_depth=10,random_state=123,oob_score=True)#start=time.perf_counter()rnd.fit(X_train,y_train)end=time.perf_counter()print(&quot;Random Forest&quot;)print(&quot;Training costs:&quot;,end-start,&quot;s&quot;)rnd_y_pre=rnd.predict(X_test)print(&quot;Prediction accuracy&quot;,accuracy_score(y_test,rnd_y_pre))#===================================#rnd_y_predict=rnd.predict_proba(X_test)print(&quot;AUC score：&quot;,roc_auc_score(y_test,rnd_y_predict[:,1]))print(&quot;Confusion matrix\\n&quot;,confusion_matrix(y_test,rnd_y_pre))print(&quot;Out of bag score；&quot;,rnd.oob_score_)The accuracy ends up with 0.98 which is almost perfect.Cross-validationFinally we check the model with k-fold validation:from sklearn.model_selection import cross_val_scorescores=cross_val_score(rnd,X_train,y_train,cv=5)print(&#39;Scores:&#39;,scores)print(&#39;Average scores:&#39;,scores.mean())Result analysisThe last part we may want to see which feature contributes the most in prediction. This is important for the explainability of the model.feature_names=list(air_data_.columns)res =sorted(zip(map(lambda x: round(x, 4),rnd.feature_importances_), feature_names), reverse=True) names=[feature[1] for feature in res[:15]]importances=[feature[0] for feature in res[:15]]plt.figure(figsize=(15,6)) plt.bar(x=0,bottom=np.arange(len(names),0,-1), height=0.5, width=importances, orientation=&quot;horizontal&quot;)plt.yticks(np.arange(len(names),0,-1),names)plt.title(&quot;Importance of features&quot;)plt.show()Aha, the days_from_last_to_end and max_flight_interval rank top 2.This project goes through almost the entire process of data analysis and applying machine learning in prediction. The final score looks pretty good. However, I know in real life the case will be much more complex and lots of information could be missing because of certain regulations or individual wills. The prediction is also not that easy to make. In a word, this is an introductory example of how to use machine learning in analysis. In my opinion, sometimes the core of data analysis is not complex models or algorithms but the business insights and the interpretation of the results based on one’s accumulated experience and intuition. That is what really matters.I put all code in jupyter notebook and store it here." }, { "title": "DHH22 Hackathon Project", "url": "/posts/Hackathon22/", "categories": "English, Projects", "tags": "Projects", "date": "2022-05-30 22:42:00 +0300", "snippet": "Project BackgroundThe project is a topic of Helsinki Digital Humanities Hackathon 2022.The theme is Network of Parlamint. The data include national parliament speeches of 17 european countries. We eventually selected and focus on the Spanish term 12, UK term 57, Slovenian term 7 datasets.Research question How is the speeches of MPs and mentions of them are related to the power? Is there power distribution between male and female speeches, regarding different topics.My workMy work is mainly about the summary statistics preparations, speech flows of MPs and other visual aids.Final outputOur team produced an excellent academic poster. Here is the picture.My experienceWhat I learned from the project. (coding practice, social science view, xxx)DocumentationGithub repository/Google Drive storing all relevant files" }, { "title": "PCA &amp; SVD", "url": "/posts/SVD-PCA/", "categories": "English", "tags": "math", "date": "2022-05-27 22:55:00 +0300", "snippet": "PCA detailsIn machine learning, threre are many features can be used for training. For example, we have 72 features in each instance and 1000 pieces of data. So now we have a matrix D with shape of (1000, 72). If we want to reduce the feature number, then we need another matrix R and compute dot product: $D * R = D^{‘}$ Suppose the shape of R is (72, 30), and the $D^{‘}$ is in (1000, 30). So the now our task is to find a good $R$ to keep as much information as possible.Maximum variancePic from Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong, 2020We can see if wee want to reduce dimension form 2 to 1, the best line is what is shown in the picture. This is understandable because the projections of dots are more discrete. Let’s consider an opposite case, say the line is perpendicular to the original one. Then the projects lie on a very narrow area, for example, they are [1, 0] [1.5, 0], [1, 0] [-1, 0] [1, 0] [-0.5, 0], which are ineffective to represent original points because a lot of them look exactly the same.Therefore, we wish the projection has the max variance so that most information is kept.CovarianceBased on above discussion, we should be able to know the task is to find a plane where the variance is max. Variance formula is as follows\\[\\begin{equation}S = \\frac{1}{N-1} \\Sigma_{i}^{N} (x_{i}-\\bar{x})^{2}\\end{equation}\\]Where N is the number of dimensions.If we change the equation a little bit, then it turns to $Cov(x,y)=\\frac{1}{N-1} \\Sigma_{i}^{N} (x_{i}-\\bar{x})*(y_{i}-\\bar{x})$ which is a covairance matrix. When x = y the value is actually the variance of x.Before we do the projection, we must first standardize the data, then do the projection and maximize the variance.\\[\\begin{equation}\\begin{aligned} J &amp;amp;= \\frac{1}{N-1}\\Sigma_{i=1}^{N} ((x_{i}-\\bar{x})\\mu)^{2}\\\\ &amp;amp;= \\frac{1}{N-1}\\Sigma_{i=1}^{N} \\mu^{T}(x_{i}-\\bar{x})^{T}(x_{i}-\\bar{x})\\mu \\\\ &amp;amp;= \\mu^{T} \\frac{1}{N-1}\\Sigma_{i=1}^{N} (x_{i}-\\bar{x})^{T}(x_{i}-\\bar{x})\\mu \\\\ &amp;amp;= \\mu^{T}S\\mu\\end{aligned}\\end{equation}\\]Where $\\mu$ is the $R$ we mentioned in previous section. There is another property for $\\mu$: $\\mu^{T}\\mu = 1$ which results from the fact that covariance matrix is always a systematric square.Then the whole question becomes an optimization problem. To find the best $\\mu$ so that the $J$ is maximum variance.\\[argmax (J) = \\mu^{T}S\\mu\\]\\[s.t. \\: \\mu^{T}\\mu = 1\\]Use Lagrange multiplier to solve the problem.\\[L(\\mu, \\lambda) = \\mu^{T}S\\mu +\\lambda(1-\\mu^{T}\\mu) \\\\\\]The equation is max when the following requirement(just one of them, but enough) is met.\\[\\frac{\\partial L}{\\partial \\mu} = 2S\\mu - 2\\lambda \\mu =0\\]The result is $S\\mu = \\lambda \\mu$ which indicates $\\lambda$ stands for eigenvalues of $S$, and $\\mu$ for eigenvectors.By computing eigenvectors corresponding to top X largest eigenvalues, we obtain the $\\mu$ to reduce dimensions.Eigenvalue &amp;amp; EigenvectorDefinition: $A x = \\lambda x$ =&amp;gt; $|A - \\lambda E|=0$The $\\lambda$ holds the equation is eigenvalue and the $x$ is eigenvector. $A$ should be a square matrix. If $A(n,n)$ is not invertible ($A$ is singular, rank is less than n), there must be at least one $\\lambda$ is zero.For example, now square matrix\\[A=\\left[ \\begin{matrix} 12 &amp;amp; 8 \\\\ 5 &amp;amp; 6 \\\\ \\end{matrix} \\right]\\]\\[\\begin{aligned}|A - \\lambda I| &amp;amp;= det \\left\\{ \\left[ \\begin{matrix} 12-\\lambda &amp;amp; 8 \\\\ 5 &amp;amp; 6-\\lambda \\\\ \\end{matrix} \\right] \\right\\} \\\\&amp;amp;=(12-\\lambda)(6-\\lambda)-5*8 \\\\&amp;amp;= \\lambda^{2} - 18\\lambda + 72 - 40 \\\\&amp;amp;= \\lambda^{2} - 18\\lambda + 32 \\\\&amp;amp;= (\\lambda -16)(\\lambda -2)\\\\\\end{aligned} \\\\\\]So the $\\lambda_{1}=2, \\lambda_{2}=16$ and the $x_{1}=\\begin{bmatrix} -4 \\ 5 \\end{bmatrix}$ and $x_{2}=\\begin{bmatrix} 2 \\ 1 \\end{bmatrix}$But here is a restriction that the matrix has to be a N x N matrix.However, in reality there are many matrices in shape of N x M.SVDSingular value and vectorConsequently, singular values and vectors are designed for those matrices whose rows(N) are different from columns(M).Suppose we can decompose $A=U\\Sigma V^{T}$, where $U$ and $V$ are both orthogonal. Then the followings also hold $AA^{T}=U\\Sigma V^{T}(U\\Sigma V^{T})^{T}=U\\Sigma V^{T}V\\Sigma U^{T}=U\\Sigma \\Sigma U^{T}$ $A^{T}A=(U\\Sigma V^{T})^{T}U\\Sigma V^{T}=V\\Sigma U^{T}U\\Sigma V^{T}=V\\Sigma \\Sigma V^{T}$So the singular values are the root square of $\\Sigma$ values.For example,\\(A=\\left[ \\begin{smallmatrix} 3 &amp;amp; 2 \\\\ 2 &amp;amp; 3 \\\\ 2 &amp;amp; -2 \\\\ \\end{smallmatrix} \\right]\\)and\\(A^{T}=\\left[ \\begin{smallmatrix} 3 &amp;amp; 2 &amp;amp; 2 \\\\ 2 &amp;amp; 3 &amp;amp; -2 \\\\ \\end{smallmatrix} \\right]\\)We first calculate the $V$\\[\\begin{aligned}A^{T}A &amp;amp;=\\left[ \\begin{matrix} 3 &amp;amp; 2 &amp;amp; 2 \\\\ 2 &amp;amp; 3 &amp;amp; -2 \\\\ \\end {matrix}\\right]*\\left[ \\begin{matrix} 3 &amp;amp; 2 \\\\2 &amp;amp; 3 \\\\ 2 &amp;amp; -2 \\\\\\end{matrix}\\right] \\\\&amp;amp;= \\left[ \\begin{matrix} 17 &amp;amp; 8 \\\\ 8 &amp;amp; 17 \\\\\\end {matrix}\\right]\\end{aligned}\\]$det(A^{T}A)=\\lambda^{2}-34\\lambda+289-64=(\\lambda -9)(\\lambda-25)$so the singular value $q_{1}=\\sqrt 9 = 3$ and $q_{2}=\\sqrt 25 = 5$ and we rank the singular values:\\(v_{1}=\\frac{1}{\\sqrt 2} \\begin{bmatrix} 1 \\\\ -1 \\\\ \\end{bmatrix}, v_{2}=\\frac{1}{\\sqrt 2} \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} , V=\\begin{bmatrix} 1 &amp;amp; 1 \\\\ 1 &amp;amp; -1 \\\\ \\end{bmatrix}, \\Sigma= \\begin{bmatrix} 5 &amp;amp; 0 \\\\ 0 &amp;amp; 3 \\\\ \\end{bmatrix}\\)According to $U=AV\\Sigma^{-1},\\; u_{i}=\\frac{Av_{i}}{\\sigma_{i}}$, we know:\\[\\begin{aligned}u_{1}&amp;amp;=\\frac{\\left[ \\begin{matrix} 3 &amp;amp; 2 \\\\2 &amp;amp; 3 \\\\ 2 &amp;amp; -2 \\\\\\end{matrix}\\right]*\\frac{1}{\\sqrt 2}\\left[ \\begin{matrix} 1 \\\\ 1 \\\\\\end {matrix}\\right]}{5} &amp;amp;=\\frac{1}{\\sqrt 2}\\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\end {matrix}\\right] \\\\u_{2}&amp;amp;=\\frac{\\left[ \\begin{matrix} 3 &amp;amp; 2 \\\\2 &amp;amp; 3 \\\\ 2 &amp;amp; -2 \\\\\\end{matrix}\\right]*\\frac{1}{\\sqrt 2}\\left[ \\begin{matrix} 1 \\\\ -1 \\\\\\end {matrix}\\right]}{3} &amp;amp;= \\frac{1}{\\sqrt 2} \\left[ \\begin{matrix} 1/3 \\\\ -1/3 \\\\ 4/3 \\end {matrix}\\right]\\end{aligned}\\]And the\\[U=\\frac{1}{\\sqrt 2}\\begin{bmatrix} 1 &amp;amp; 1/3 \\\\ 1 &amp;amp; -1/3 \\\\ 0 &amp;amp; 4/3 \\\\ \\end{bmatrix}\\]And now we can verify:\\[U\\Sigma V^{T}=\\frac{1}{\\sqrt 2}\\begin{bmatrix} 1 &amp;amp; 1/3 \\\\ 1 &amp;amp; -1/3 \\\\ 0 &amp;amp; 4/3 \\\\ \\end {bmatrix}* \\begin{bmatrix} 5 &amp;amp; 0 \\\\ 0 &amp;amp; 3 \\\\ \\end{bmatrix} * \\frac{1}{\\sqrt 2} \\begin{bmatrix} 1 &amp;amp; 1 \\\\ 1 &amp;amp; -1 \\\\ \\end{bmatrix} =\\frac{1}{2} \\begin{bmatrix} 6 &amp;amp; 4 \\\\ 4 &amp;amp; 6 \\\\ 4 &amp;amp; -4 \\\\ \\end{bmatrix}=A\\]ApplicationSo in which way can it be used?The general form of SVD is:\\(\\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \\cdots &amp;amp; a_{1m} \\\\ a_{21} &amp;amp; a_{22} &amp;amp; \\cdots &amp;amp; a_{2m} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots \\\\ a_{n1} &amp;amp; a_{n2} &amp;amp; \\cdots &amp;amp; a_{nm}\\\\\\end {bmatrix}_{(n \\times m)} = \\left[ \\begin{matrix} u_{11} &amp;amp; \\cdots &amp;amp; u_{1k} \\\\ \\vdots &amp;amp; \\ddots &amp;amp; \\vdots\\\\ u_{n1} &amp;amp; \\cdots &amp;amp; u_{nk}\\\\\\end {matrix}\\right]_{(n \\times k)} \\left[ \\begin{matrix} \\sigma_{1} &amp;amp; 0 &amp;amp; \\cdots &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; \\sigma_{2} &amp;amp; \\cdots &amp;amp; 0 &amp;amp; 0\\\\ \\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots &amp;amp; \\vdots \\\\ 0 &amp;amp; 0 &amp;amp; \\cdots &amp;amp; \\sigma_{k-1} &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; \\cdots &amp;amp; 0 &amp;amp; \\sigma_{k}\\\\\\end {matrix}\\right]_{(k \\times k)}\\left[ \\begin{matrix} v_{11} &amp;amp; \\cdots &amp;amp; v_{1k} \\\\ \\vdots &amp;amp; \\ddots &amp;amp; \\vdots\\\\ v_{m1} &amp;amp; \\cdots &amp;amp; v_{mk}\\\\\\end {matrix}\\right]^{T}_{(k \\times m)}\\)where $\\sigma_{i}$ is ranked by value from high to low, which indicates the contribution of $\\sigma_{i}$.Sometimes the last several values like $\\sigma_{k}$ are quite small/close to 0, and they can be just removed. And the form is:\\[U^{&#39;}_{(n \\times j)}\\Sigma^{&#39;}_{(j \\times j)} V^{&#39;T}_{(j \\times m)}\\]This way of decomposition saves the most information of matrix.Python codeBy running the following code, it shows the original picturefrom PIL import Imageimport numpy as npimport matplotlib.pyplot as plttower=Image.open(&quot;SVD-origin.jpg&quot;,&quot;r&quot;)# file name could be replacedtower_pixel_matrix=np.asarray(tower)plt.figure(figsize=(9, 9))plt.imshow(tower)Rhine Tower, Dusseldorf (So beautiful!)x,y,z=tower_pixel_matrix.shapetower_pixel_matrix = tower_pixel_matrix.reshape(x,y*z)U, sigma, V=np.linalg.svd(tower_pixel_matrix)fig,ax=plt.subplots(nrows=2, ncols=2,figsize=(12,8))for k in [(0,0,10),(0,1,20),(1,0,100),(1,1,500)]: blurred_tower = np.matrix(U[:, :k[2]]) * np.diag(sigma[:k[2]]) * np.matrix(V[:k[2], :]) ax[k[0],k[1]].imshow(blurred_tower) ax[k[0],k[1]].title.set_text(&quot;Key components:{}&quot;.format(k[2]))Picture compressionIt’s clear that key components save the most important information, like the outline of objects. The more components are saved, the more details are provided.sklearn provides API for SVD as well. This approach is usually applied to reduce dimensions of data at the least cost. It’s a very useful method in machine learning. This method can also be used in NLP field. For example, Latent Semantic Analysis(LSA) uses SVD to map the term-document on certain space and grouping words of similar semantic meaning.Reference Linear Algebra and Learning from Data (2019) Image compression idea Mathematics for machine learning" }, { "title": "Chinese word segmentation statistical approach(I)", "url": "/posts/unigram-word-segmentation/", "categories": "English", "tags": "NLP, word-segmentation", "date": "2022-05-25 22:42:00 +0300", "snippet": "Language model is a model predicting probabilities of sentences or sequences of words.This blog discuss about how to do word segmentation. Unlike European languages, some Asian languages like Chinese and Janpanese, don’t have spaces between each words. Humans understand languages because we automatically have recognized words in heads. For machinese, those are merely sequences of symbols and characters. If we hope computers to correctly deal with them, word segmentation is a necessary step. Here the blog is going to discuss a statistical approach, N-gram.It starts with unigram (1-gram) which sees every word is independent from context. So with training data, the probability of a word is calculated as:\\(P(word)=\\frac{C(word)}{N}\\)where N is the total number of words. But there would be some exceptions that some characters or words never show up in the corpus but it doesn’t mean the word doesn’t exist, does it? Therefore, we adopt plus 1 and V on numerator and denominator. The equation becomes:\\(P(word)=\\frac{C(word)+1}{N+|V|}\\)where |V| is unique type of words since the smoothing way think every word should at least show up for once.Since all preparing steps are done, now the task is to find the segmentation way with highest probabilities.This could be done by greedy algorithm(local optimum) or other global optimum way.This unigram method isolates each words and ignore contextual words. It simply select words according to their frequencies. If we use it as a model for language generation, the produced sentence would definitely quite fragmented and incoherent. Thus, it’s not a very good solution for this task. Nevertheless, don’t forget the N can be larger and more information is taken into account.Reference: Speech and Language Processing" }, { "title": "Chinese word segmentation statistical approach(II)", "url": "/posts/bigram-word-segmentation/", "categories": "English", "tags": "NLP, word-segmentation", "date": "2022-05-25 22:42:00 +0300", "snippet": "IntroductionThis blog introduces the bigram model for word segmentation.So the bigram model considers previous character which makes more sense in terms of context.The equation should be:\\(P(W_{i}|W_{i-1})=\\frac{C(W_{i},W_{i-1})}{C(W_{i-1})}\\)However, we may also wonder what if $W_{i-1}$ is zero? The probability cannot be computed in this case. The combination of $W_{i}$ and $W_{i-1}$ can be zero as well.Witten-Bell smoothingThus, to solve this problem, we need some smoothing and interpolation is introduced.\\(P^{&#39;}(W_{i}|W_{i-1})=\\frac{C(W_{i-1})}{N_{1+}(W_{i-1})+C(W_{i-1})}*P(W_{i}|W_{i-1})+\\frac{N_{1+}(W_{i-1})}{N_{1+}(W_{i-1})+C(W_{i-1})}*P(W_{i})\\) sourceThis smoothing trick combines both bigram and unigram. There are two coefficient, $\\lambda_{1}$ and $\\lambda_{2}$. First, we make sure $\\lambda_{1}+\\lambda_{2}=1$ and the $\\lambda$ decides which part contributes more to the final probability. Here $N_{1+}(W_{i-1})$ means words following $W_{i-1}$ at least once. In fact, it measures the freedom/flexibility of collocation. For example, Hong Kong, so the word following Hong is mainly Kong. In this case, the collocation is very strong and the proportion of $W_{i}$ as an independent word should be lower, and the conditional probability therefore becomes higher. That is to say, the word is quite dependent on its previous word. Another example, red apple. Since red is just an adjective and almost any nouns can be modified by it. The value of $N_{1+}(W_{i-1})$ is supposed to be pretty high. More importantly, if the value of conditional probability is zero, we still have unigram part so that the probability won’t be zero. Of course, in terms of preventing the $C(W_{i-1})$ from being zero, plus 1 can easily fix it.So far the probabilities are calculable. The workflow would be like (赫尔辛基大学在芬兰[The university of Helsinki is in finland/Helsingin yliopisto on suomessa]): Find best segmentation wayYes, next step requires ability to search for the optimal path in this graph. Probabilities will serve as weights of edges.In practice, adding regular expression would be a good idea to recognize time and numbers, boosting the performance on OOV(out of vocabulary). Implementation is in the repository. The blog is just complementary to the code.Here we can also see some drawbacks of this approach: It relies on large training data. N-gram usually performs better as N grows but also more computationally expensive. It still can’t deal with unseen words which never appear in training data. (Regular expression only helps some but can’t manage person’s name)HMM model seems to be more effective in tackling OOV.Reference: Speech and Language Processing" }, { "title": "Parsing mathematical expressions with tree structure", "url": "/posts/Binary-Tree/", "categories": "English", "tags": "data-structure-algorithm", "date": "2022-05-24 22:00:00 +0300", "snippet": "Credit to Problem solving with algorithms and data structures using Python section 7.5 &amp;amp; 7.6 &amp;amp; 7.7The idea is to use binary tree to parse mathematical tree. For example, (3*(2+5)) the tree should be like:First we need to have a class for binary tree.# some may be modifiedclass BinaryTree: def __init__(self,rootobj): self.key=rootobj self.leftchild=None self.rightchild=None def insertLeft(self,newnode): if self.leftchild==None: self.leftchild=BinaryTree(newnode) else: t = BinaryTree(newNode) t.leftChild = self.leftChild self.leftChild = t def insertRight(self,newnode): if self.rightchild==None: self.rightchild=BinaryTree(newnode) else: t = BinaryTree(newNode) t.rightChild = self.rightChild self.rightChild = t def getRightChild(self): return self.rightchild def getLeftChild(self): return self.leftchild def setRootVal(self,value): self.key=value def getroot_value(self): return self.keyinsertLeft()function check if the left subnode exists, if not then create another new Binary tree class as the leftchild node. If there’s already something, then creating a new node and put the original leftchild as the leftchild node of the new one. Now the new Binary tree class is the leftchild node. Same for insertRight().Then we need to have a function to build a tree and parse this type of expressions.# some lines are modifieddef buildParseTree(fpexp): fplist = [i for i in fpexp] pStack = [] eTree = BinaryTree(&#39;&#39;) pStack.append(eTree) # equuivalent of pushing into stack currentTree = eTree for i in fplist: if i == &#39;(&#39;: currentTree.insertLeft(&#39;&#39;) pStack.append(currentTree) currentTree = currentTree.getLeftChild() elif i in [&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;]: currentTree.setRootVal(i) currentTree.insertRight(&#39;&#39;) pStack.append(currentTree) currentTree = currentTree.getRightChild() elif i == &#39;)&#39;: currentTree = pStack.pop() elif i not in [&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;, &#39;)&#39;]: currentTree.setRootVal(int(i)) parent = pStack.pop() currentTree = parent else: raise ValueError(&quot;token &#39;{}&#39; is not a valid integer&quot;.format(i)) return eTreeimport operator as opdef evaluate(tree): operations={&quot;+&quot;:op.add,&quot;-&quot;:op.sub,&quot;*&quot;:op.mul,&quot;/&quot;:op.truediv} leftC = tree.getLeftChild() rightC = tree.getRightChild() if leftC and rightC: return operations[tree.getroot_value()](evaluate(leftC),evaluate(rightC)) else: return tree.getroot_value()def preorder(tree): if tree: print(tree.getroot_value()) preorder(tree.getLeftChild()) preorder(tree.getRightChild())preorder(buildParsetree(&quot;(3*(2+5))&quot;))evaluate(buildParseTree(&quot;(3*(2+5))&quot;))The result is 3*7=21 and by preorder function, we have all nodes in the tree which in order are */3/+/2/5.Things seem to have been solved so far. But I wonder if some parentheses are necessary in the expression.Here I have two more examples: 1) 2+(3*5) 2) (3+5)*2And this time I don’t want the outside parenthesis.In order to do this, I made some modifications:elif i not in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;, &quot;)&quot;]:# number if currentTree.getroot_value()==&quot;&quot;:# number without parentheses currentTree.insertleft(&quot; &quot;) node_stack.append(currentTree) currentTree = currentTree.getLeftchild() currentTree.setroot(int(i)) parent = node_stack.pop() currentTree = parentIf the root value is ““(empty string) which means there is no operator encountered, then the number should be automatically put on the left side of current tree.Also, another issue is that if the operator is behind a parenthesis, it will replace the current operator. In this case, the whole subtree should be a leftchild node of a new Binary tree class.So we need to have a new subtree and keep the original one as the leftchild.# add this function into the BinaryTree classdef replacecurr(self,newnode): t_r = self.rightchild t_l = self.leftchild self.leftchild = BinaryTree(self.key) self.rightchild = None self.leftchild.leftchild = t_l self.leftchild.rightchild = t_r#change the buildParseTree functionelif p in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;]: if currentTree.getroot_value() in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;, &quot;)&quot;]:# calculation symbol but without priority currentTree.replacecurr(p) currentTree.setroot(p) currentTree.insertright(&quot; &quot;) node_stack.append(currentTree) currentTree = currentTree.getRightchild()Thus, the output of 2+(35)=17, and the nodes are +/2/*/3/5.Then the second example (3+5)2=16 and nodes are */+/3/5/2.Good, the uncessary parenthesis issue looks solved. However, I come up with another example, what will happen for 2+(3*5)/3?evaluate(buildParseTree(&quot;2+(3*5)/3&quot;))Oops, the result is 5.667. The new problem is that we don’t have any priority for the calculation so the program see the expression as (2+(3*5))/3 which equals to 17/3=5.667So we need some priorities to put * and \\ prior in calculation.The final entire codes will be like:(look kind of messy)class BinaryTree: def __init__(self,rootobj): self.key=rootobj self.leftchild=None self.rightchild=None def insertLeft(self,newnode): if self.leftchild==None: self.leftchild=BinaryTree(newnode) else: t = BinaryTree(newNode) t.leftChild = self.leftChild self.leftChild = t def insertRight(self,newnode): if self.rightchild==None: self.rightchild=BinaryTree(newnode) else: t = BinaryTree(newNode) t.rightChild = self.rightChild self.rightChild = t def replacecurr(self,newnode): t_r = self.rightchild t_l = self.leftchild self.leftchild = BinaryTree(self.key) self.rightchild = None self.leftchild.leftchild = t_l self.leftchild.rightchild = t_r def getRightChild(self): return self.rightchild def getLeftChild(self): return self.leftchild def setRootVal(self,value): self.key=value def getroot_value(self): return self.keydef buildParseTree(fpexp): fplist = [i for i in fpexp] pStack = [] eTree = BinaryTree(&#39;&#39;) pStack.append(eTree) # equuivalent of pushing into stack currentTree = eTree for i in fplist: if i == &#39;(&#39;: currentTree.insertLeft(&#39;&#39;) pStack.append(currentTree) currentTree = currentTree.getLeftChild() elif i in [&quot;+&quot;, &quot;-&quot;]: if currentTree.getroot_value() in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;, &quot;)&quot;]:# calculation symbol but without priority currentTree.replacecurr(i) currentTree.setRootVal(i) currentTree.insertRight(&quot; &quot;) pStack.append(currentTree) currentTree = currentTree.getRightChild() elif i in[&quot;*&quot;, &quot;/&quot;]: if currentTree.getroot_value() in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;, &quot;)&quot;]:# calculation symbol but without priority pStack.append(currentTree) currentTree = currentTree.getRightChild() currentTree.replacecurr(i) currentTree.setRootVal(i) currentTree.insertRight(&quot; &quot;) pStack.append(currentTree) currentTree = currentTree.getRightChild() elif i == &#39;)&#39;: currentTree = pStack.pop() elif i not in [&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;, &#39;)&#39;]: if currentTree.getroot_value()==&quot;&quot;:# number without parentheses currentTree.insertLeft(&quot; &quot;) pStack.append(currentTree) currentTree = currentTree.getLeftChild() currentTree.setRootVal(int(i)) parent = pStack.pop() currentTree = parent # in case of consecutive operator and can&#39;t go back to root node if currentTree.getroot_value() in [&quot;*&quot;,&quot;/&quot;] and currentTree.getLeftChild().getroot_value() in [&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;, &quot;)&quot;]: parent = pStack.pop() currentTree = parent else: raise ValueError(&quot;token &#39;{}&#39; is not a valid integer&quot;.format(i)) return eTreeimport operator as opdef evaluate(tree): operations={&quot;+&quot;:op.add,&quot;-&quot;:op.sub,&quot;*&quot;:op.mul,&quot;/&quot;:op.truediv} leftC = tree.getLeftChild() rightC = tree.getRightChild() if leftC and rightC: return operations[tree.getroot_value()](evaluate(leftC),evaluate(rightC)) else: return tree.getroot_value()def preorder(tree): if tree: print(tree.getroot_value()) preorder(tree.getLeftChild()) preorder(tree.getRightChild())preorder(buildParseTree(&quot;2+(3*5)/3&quot;))evaluate(buildParseTree(&quot;2+(3*5)/3&quot;))Great! The result is finally correct: 2+5=7.0" }, { "title": "Graph and shortest path", "url": "/posts/Graphs-shortst-path/", "categories": "English", "tags": "data-structure-algorithm", "date": "2022-05-24 20:22:00 +0300", "snippet": "About Graph, there are several basic concepts: Node, as a point in the graph. Edge/Arc, meaning two nodes are connected. Weight, the value/expense of edge. Path, the way including all passed nodes from one node to the target one. Cycle, if the path starts from one node and end with the same. Directed Graph, edges have direction, pointing to certain node. Adjacency matrix, if the graph is not directed then the matrix is symmetric.With networkx package, it’s easy to draw some example graphs. Indirected graph Weighted directed graphThere is an interesting topic about Graph: Knight’s Tour Problem. But it will not be covered in this post.I want to introduce two algorithms of finding the shortest path. That’s to discover the way at lowest cost.Dijkstra’s algorithmThe problem is that we need to find the cheapest way from node 0 to other nodes.First we need to build an adjacency matrix and then to iteratively travel each point.&quot;&quot;&quot;Dijkstra Algorithm&quot;&quot;&quot;oo=float(&#39;inf&#39;)#adjacency matrixdistant = [[oo,1,2,oo,7,oo,4, 8],# [1,oo,2,3,oo,oo,oo,7],# [2,2,oo,1,5,oo,oo,oo],# [oo,3,1,oo,3,6,oo,oo],# [7,oo,5,3,oo,4,3, oo],# [oo,oo,oo,6,4,oo,6,4],# [4,oo,oo,oo,3,6,oo,2],# [8,7,oo,oo,oo,4,2,oo]]# S=[0];U=[i for i in range(1,len(distant[0]))]lowest_cost = {0:0,1:oo,2:oo,3:oo,4:oo,5:oo,6:oo,7:oo}cost=list(lowest_cost.values())z = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0}def dijkstra(distant,n): iternum=0 while U != []:#as long as the U is not empty for i in U:#for each element in U if lowest_cost[i]&amp;gt; lowest_cost[S[iternum]]+distant[S[iternum]][i]:#if the cost is lower than two plus together, it&#39;ll be recorded as a new lowest cost lowest_cost[i]=lowest_cost[S[iternum]]+distant[S[iternum]][i] cost[i]=lowest_cost[i];z[i]=S[iternum] else: pass #always choose the minimum cost and restore it picked = min(cost[1:n+1]) found_number = cost.index(picked) cost[found_number]=oo#reset as infinitive for next round selection U.remove(found_number) S.append(found_number) iternum +=1 return lowest_costdijkstra(distant,7)for i in range(len(cost)): print(&quot;Node({0}) to Node 0 has shortest path at expense of:{1}&quot;.format(i,lowest_cost[i])) print(&#39;The best previous node of Node({0}) is:{1}&#39;.format(i,z[i]))Dijkstra algorithm computes the shortest path between a fixed node and all the other nodes. This is very helpful when we deal with some delivery issues, like there is a logistics center.Floyd algorithmThis algorithm enables us to find the shortest path of any two nodes and record the path (which nodes passed by).&#39;&#39;&#39;Floyd Algorithm&#39;&#39;&#39;import numpy as npoo=float(&quot;inf&quot;)#其实把这个调大一点也可以d_matrix=np.array([[oo,1,2,oo,7,oo,4, 8],# 这边其实就是每个点之间相互的距离 [1,oo,2,3,oo,oo,oo,7],# [2,2,oo,1,5,oo,oo,oo],# [oo,3,1,oo,3,6,oo,oo],# [7,oo,5,3,oo,4,3, oo],# [oo,oo,oo,6,4,oo,6,4],# [4,oo,oo,oo,3,6,oo,2],# [8,7,oo,oo,oo,4,2,oo]])r_matrix=np.array([[i for i in range(d_matrix.shape[0])]for k in range(d_matrix.shape[0])])def Floyd(n): for k in range(n): for i in range(n): for j in range(n): if d_matrix[i][j]&amp;gt;d_matrix[i][k]+d_matrix[k][j]: d_matrix[i][j]=d_matrix[i][k]+d_matrix[k][j] d_matrix[j][i]=d_matrix[i][k]+d_matrix[k][j] r_matrix[i][j]=k;r_matrix[j][i]=k print(d_matrix) print(r_matrix)Floyd(d_matrix.shape[0])def printpath(r_mx,fromnd,tond): nextnd=r_mx[fromnd][tond] pathnd=[fromnd] while nextnd != tond: pathnd.append(nextnd) nextnd = r_mx[nextnd][tond] pathnd.append(nextnd) print(&quot;Best path from %d to %d&quot;%(fromnd,tond),&quot;-&amp;gt;&quot;.join([str(i) for i in pathnd]))printpath(r_matrix,4,1)With Floyd’s way, we can choose any two nodes and find the shortest way of it. The example from node 4 to 1, the best path is 4-&amp;gt;3-&amp;gt;1.The main part of above codes were actually written during my undergraduate study, the course Mathematical Modeling in Economy. The content of it really helped me a lot. The example problem also comes from the course, so I should give credit to it.So what is the meaning of finding the shortest path? Not only for route planning but even quite useful for NLP, for example word segmentation, syntax parsing!" }, { "title": "Text Multilabel Classification using BERT", "url": "/posts/Text-classification-with-BERT/", "categories": "English, Projects", "tags": "Projects", "date": "2022-05-01 09:00:00 +0300", "snippet": "Project introductionIn this project, the goal is to do multilabel classification. A piece of news can be related to only one topic or several. The task is training a model to classify relevant themes of news. Therefore, this is also a multilabel classification problem.I did the work of data exploring, cleaning and turning them into batches.Then I used RoBERTa and distilled BERT to tackle the task, the latter of which is lighter but has a similar performance as original model.I also compared two models and validate them on development dataset.Text extraction &amp;amp; data explorationThe content of news as well as its label is embedded in xml files. So after downloading the file, the first thing we should do is extracting useful information from files.def read_file(file_path): file=etree.parse(&#39;./%s&#39;%file_path)# replace it with file_path return file.getroot()def extract_text(file_path): root=read_file(file_path) file_id=root.xpath(&quot;/newsitem/@itemid&quot;)[0] data[file_id]={} if root.xpath(&quot;//headline/text()&quot;)!=[]: data[file_id][&quot;HEADLINE&quot;]=root.xpath(&quot;//headline/text()&quot;)[0].capitalize() else: data[file_id][&quot;HEADLINE&quot;]=&#39;&#39; data[file_id][&quot;TEXT&quot;]=txt_clean(&#39; &#39;.join(root.xpath(&quot;//text/p/text()&quot;))) # data[file_id][&quot;TEXT&quot;]=&#39; &#39;.join(root.xpath(&quot;//text/p/text()&quot;)).strip() data[file_id][&quot;LABEL&quot;]=root.xpath(&quot;//codes[@class=&#39;bip:topics:1.0&#39;]/code/@code&quot;)for directory in filedir: print(&quot;%s directory completed&quot;%directory) for file in os.listdir(&#39;../src-data/%s&#39;%directory): extract_text(&#39;../src-data/%s/%s&#39;%(directory,file))I use lxml.etree parsing the format and xpath syntax to obtain contents and labels.Then do some data explorations. I sort of want to see how different labels are distributed; if they are unevenly labeled? What’s the most frequent label in thousands of texts.df_lb=pd.DataFrame(code_label)df_lb.to_csv(&quot;../data_files/label-codes.csv&quot;)array_label = df[&quot;label&quot;].apply(lambda x: np.array(x))# this corresponds to the 3rd figure in report, 10 most frequent labelstopN = 10freq_index = (np.argsort(sum(array_label))[::-1][:topN],np.argsort(sum(array_label))[::-1][-topN:])mostfreq_classes = [code_label[&quot;code&quot;][i] for i in freq_index[0]]leastfreq_classes = [code_label[&quot;code&quot;][i] for i in freq_index[1]]most_frequency = sorted(sum(array_label),reverse=True)[:topN]least_frequency = sorted(sum(array_label),reverse=True)[-topN:]most_label=pd.DataFrame({&quot;most_classes&quot;:mostfreq_classes,&quot;frequency&quot;:most_frequency})least_label=pd.DataFrame({&quot;least_classes&quot;:leastfreq_classes,&quot;frequencies&quot;:least_frequency})label_data=pd.concat([most_label,least_label],axis=1)The frequency of labels turns out to be pretty imbalanced.Take a look at the following picture, we can see the most frequent label CCAT.And if we compare the top 10 most frequent labels and 10 least frequent ones, we see there is a really large gap.So this issue could lead the model to focus more on the frequent labels instead of rare tags.Another discovery is that there are totally 126 different topics. Thereby the output of the neural model should be a vector including 126 elements standing for possiblities of the topics.How about the texts? The maximum tokens BERT model can deal with as input is 512. So if text length exceeds the limitation too much, only part of contents will be processed.OK, we can see majorities are within 512 tokens.Data transformation and preparation Since the label is in format of string. For example, [‘CCAT’, C15’’] or [‘M14’].In order to convert the labels into a format we can use for calculating loss, it should be in one-hot encoding.#transforming labels into one-hot encodingfrom sklearn.preprocessing import MultiLabelBinarizermlb = MultiLabelBinarizer(classes=df_lb[&quot;code&quot;])def labels2binaries(label,model=mlb): return model.fit_transform(label)onehot_label = labels2binaries(df[&quot;LABEL&quot;])df[&quot;label&quot;] = onehot_label.tolist()df[[&#39;text&#39;,&#39;label&#39;]].to_csv(&quot;../data_files/lower_nosep_data.csv&quot;,index=False) Preparing dataThe texts are still strings while machine doesn’t recognize. For machine, each word is represented by an id.What’s more, we usually use mini-batch for training in practice, allowing the process running in parallel.With the help of Dataset and pre-trained tokenizer, it’s not hard to achieve those goals.from datasets import Datasetfrom transformers import RobertaTokenizerfrom torch.utils.data import DataLoaderTokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)class CustomDataset(Dataset): def __init__(self, Data, tokenizer, max_len): self.tokenizer = tokenizer self.allinfo = Data self.text = self.allinfo[&#39;text&#39;] self.labels = self.allinfo[&quot;label&quot;] self.max_len = max_len def __len__(self): return len(self.text) def __getitem__(self, index): inputs = self.tokenizer.encode_plus( self.text[index], None, add_special_tokens=True, max_length=self.max_len, padding=&#39;max_length&#39;, return_token_type_ids=True, truncation=True ) ids = inputs[&#39;input_ids&#39;] mask = inputs[&#39;attention_mask&#39;] token_type_ids = inputs[&quot;token_type_ids&quot;] return {&#39;ids&#39;: torch.tensor(ids, dtype=torch.long),&#39;mask&#39;: torch.tensor(mask, dtype=torch.long), &#39;token_type_ids&#39;: torch.tensor(token_type_ids, dtype=torch.long), &#39;targets&#39;: torch.tensor(self.labels[index], dtype=torch.float)}Here CustomDataset inherit from its parent class–Dataset. We, however, need to rewrite some methods.There is one thing worth to mention:in encode_plus(), parameter add_special_tokens=True, meaning every time at the beginning of a sentence, there will be a special token [CLS] and at the end [SEP];padding=True means those texts whose tokens are less than max tokens will be filled with special tokens. Thereby sentence length is always the same.truncation=True is to make sure the sentence length won’t beyond the limitation.There might be one more confusing thing about the code–What is mask? Mask is to prevent the model from seeing the true id and let it guess what’s the true word. This is implemented in pre-training round of BERT.Building modelFirst block is BERT, and feed the model with input id list.And then pass the sentence embedding into a feed-forward network and predict corresponding labels.class BERTClass(torch.nn.Module): def __init__(self): super(BERTClass, self).__init__() self.l1 = transformers.DistilBertModel.from_pretrained(&#39;distilbert-base-uncased&#39;, output_hidden_states=False) self.l2 = torch.nn.Sequential( torch.nn.Linear(768, 768), torch.nn.ReLU(), torch.nn.Dropout(0.1), torch.nn.Linear(768, 126), ) def forward(self, ids, mask): output_1 = self.l1(ids, attention_mask=mask) output = self.l2(output_1[0]) output = output[:, 0, :].squeeze() return outputmodel = BERTClass().to(device)def loss_fn(outputs, targets): return torch.nn.BCEWithLogitsLoss()(outputs, targets)TrainingThen we apply the model in training. The step consists of getting batch data, training &amp;amp; saving the model and finally evaluating the accuracy.def train(epoch): model.train() fin_targets = [] fin_outputs = [] total_loss = 0 for _, data in enumerate(Data_loading.train_loader, 0): ids = data[&#39;ids&#39;].to(device, dtype=torch.long) mask = data[&#39;mask&#39;].to(device, dtype=torch.long) # print(torch.max(ids),torch.min(ids)) # print(torch.max(mask),torch.min(mask)) # token_type_ids = data[&#39;token_type_ids&#39;].to(device, dtype = torch.long) targets = data[&#39;targets&#39;].to(device, dtype=torch.float) outputs = model(ids, mask) optimizer.zero_grad() loss = loss_fn(outputs, targets) total_loss += loss.item() fin_targets.extend(targets.cpu().detach().numpy().tolist()) fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist()) optimizer.zero_grad() loss.backward() optimizer.step() if _ % 500 == 0 and _ != 0: print(&#39;Epoch: {}, batch:{}, Avg Loss: {}&#39;.format(epoch+1, _, total_loss/(_+1)), flush=True) checkpoint = { &#39;epoch&#39;: epoch + 1, &#39;state_dict&#39;: model.state_dict(), &#39;optimizer&#39;: optimizer.state_dict()} return fin_targets, fin_outputs, total_loss/len(Data_loading.train_loader), checkpointThe training process takes quite a long time. For RoBERTa model, about 20+ hours is spent. As expected, the distilled model has a faster training step, with about 12 hours.The validation step basically remains the same but doesn’t save models.#!/bin/bash#SBATCH -n 1 # node you request#SBATCH -p gpu # use gpu#SBATCH -t 20:00:00 # time for gpu#SBATCH --mem= Memory size (GB)#SBATCH --gres=gpu:v100:1 #SBATCH -J name of the project#SBATCH -o &amp;lt;outcome file path&amp;gt;#SBATCH -e &amp;lt;error file path&amp;gt;#SBATCH --account= &amp;lt;your project id&amp;gt;#SBATCH --mail-type=ALL#SBATCH --mail-user= &amp;lt;your email address&amp;gt;module purgemodule load pytorchpython train.pyAt the university, I use Puhti to enable GPU training and above show some bash commands and configurations.EvaluationThe following plot is the performance of distlBERT.After several epochs, the model reaches about 70% in accuracy.Here the way of calculating accuracy is somewhat different from usual one. The model could miss one true topic or predict a wrong topic. The accuracy computation is achieved via sklearn.metrics.accuracy_score.Only if the predicted labels match exactly the true values, will it be counted as a correct prediction. There are quite a few types of topics, so each topic receives an accuracy value. The final one is the averaged value. In fact, I also applied f1 score (both micro and macro).The results show that RoBERTa performs better than DistlBERT. Although, the compressed model falls behind 1~2% in accuracy, it indeed saves more time.Project reviewOverall in this project, I go through the pipeline of solving a NLP problem.I familiarized with the necessary steps of text classification. Also, I had hands-on experience of using BERT model.There are actually more things can be done to improve the results. For example, text cleaning or how to deal with the imbalanced labels. Efforts of data augmentation should be made as well. Some codes are adopted from the internet. The followings are some sites I referred while doing the project.BERT, RoBERTa, DistilBERT, XLNet — which one to use?Transformers for Multi-Label Classification made simpleMulti-label Text Classification with BERT using Pytorch" }, { "title": "Matkani Ruotsissa", "url": "/posts/Matkani-Ruotsissa/", "categories": "Suomi(Finnish)", "tags": "Language-learning", "date": "2022-03-13 18:52:00 +0200", "snippet": "Minä menin Ruotsiin lentokoneella viime viikonloppuna. Minä saavuin tukholmaan yhdeltätoista. Sitten menin bussilla kaupunkikeskukseen lentoasemalta. Lipun hinta oli kaksitoista euroa. Iltäpäivällä minä ostin t-paidan ja takkeja, ja kävelin vanhassakaupungissa. Sää oli aurinkoinen ja lämmin. Rakennukset olivat historiallisia ja kauniita. Kadulla oli paljon matkailijoita. Illalla minä söin lohikeittoa, paistettua lohta ja mansikkakakkua. Päivällinen oli herkullinen, mutta kakku oli liian makea.Seuraavana päiväna heräsin seitsemältä. Minä otin valokuvan kauniista maisemasta. Tuon jälkeen minä vierailin kolmessa museossa. Minä pidin paljon vasa-museosta koska minusta se oli niin hämmästyttävä. Minä tykkäsin myös Pohjoismaisesta museosta.Minä kävin toisissa paikoissa metrolla viimeisenä päiväna matkasta. Lensin Suomeen yhdeksältä illalla. Minusta oli hyvä olla Helsingissä uudestaan koska minä ymmärsin kun henkilökunta sanoi ”Tervetuloa”. Minä sanoin ”Moikka” lentoemännälle kun poistuin lentokoneesta.Tämä matka oli tosi kiinnostava. Minä tykkään Tukholmasta. Se on kaunis ja iso kaupunki. Minusta ruotsalaiset myös puhuvat hyvää englantia ja siksi matka oli helppo siellä.Kiitos, ystäväni Ella. Hän auttoi minua kirjoittamisessa." }, { "title": "Poisson Process", "url": "/posts/testing-mathjax/", "categories": "中文(Chinese)", "tags": "math", "date": "2020-05-01 15:00:00 +0300", "snippet": "Poisson Process泊松过程主要刻画小概率事件在一段时间内发生的情况，在排队论，等待时间，累计损耗计算方面有很大用处。泊松过程的三种定义有计数过程N(t),如果满足以下条件：\\(\\left\\{ \\begin{array}{llll} N(0)=0, 0\\;\\;time\\; happens\\; in\\; 0\\; time &amp;amp; \\\\ N(t)\\; is\\; process\\; with\\; stationary\\; independent\\; increments &amp;amp;\\\\ P\\left\\lbrace N(\\Delta t)=1\\right\\rbrace =\\lambda \\Delta t+o(\\Delta t) , \\lambda&amp;gt;0 &amp;amp; \\\\ P\\left\\lbrace N(\\Delta t)\\geq2\\right\\rbrace=o(\\Delta t) \\end{array} \\right.\\)那么 \\(\\left\\lbrace N(t),t\\geq 0\\right\\rbrace\\) 是参数为 \\(\\lambda\\) 的齐次泊松过程。有计数过程N(t),如果满足以下条件：\\(\\left\\{ \\begin{array}{lll} N(0)=0, 0 time\\, happens\\, in\\, 0 time &amp;amp; \\\\ N(t)\\; is\\, process\\, with\\, stationary\\, independent\\, increments &amp;amp;\\\\ N(t)-N(s)\\sim P(\\lambda(t-s)) \\end{array} \\right.\\)那么 \\(\\left\\lbrace N(t),t\\geq 0\\right\\rbrace\\) 是参数为 \\(\\lambda\\) 的齐次泊松过程。有更新计数过程 \\(N(t),\\;T_{n}\\) 是每一次更新间隔时间，\\(T_{1},T_{2} \\dots T_n\\) 独立同分布，那么如果满足以下条件： \\(T_{n}\\sim E(\\lambda)\\) Exponential Distr. \\(\\left\\lbrace N(t),t\\geq 0\\right\\rbrace\\) 是参数为 \\(\\lambda\\) 的齐次泊松过程。多次伯努利实验逼近泊松分布伯努利试验得到的就是二项分布，事件A发生的概率为p，不发生的概率为1-p，泊松过程实际上也是一个计数过程。有 \\({ X(t)=n}\\) . 如果我们在t时间段内，做n次伯努利试验，也就是将t时间无限切割，每个时间段 \\(\\Delta t=\\dfrac{t}{n}\\) 设t时间内有k次事件发生，2\\(\\begin{align*}P\\left\\lbrace N_{t}=k\\right\\rbrace&amp;amp;=\\binom{n}{k}(\\lambda\\Delta t)^{k}(1-\\lambda\\Delta t)^{n-k}\\\\&amp;amp;= \\dfrac{n!}{k!(n-k)!}(\\dfrac{\\lambda t}{n})^{k}(1-\\dfrac{\\lambda t}{n})^{n-k}\\\\&amp;amp;={\\lim_{n \\to +\\infty}}\\dfrac{(\\lambda t)^{k}}{k!}\\dfrac{n(n-1)(n-2)\\dots(n-k+1)}{n^{k}}(1-\\dfrac{\\lambda t}{n})^{\\frac{-n}{\\lambda t}\\frac{-\\lambda t(n-k)}{n}}\\\\&amp;amp;={\\lim_{n \\to +\\infty}}\\dfrac{(\\lambda t)^{k}}{k!}e^{\\frac{-\\lambda t (n-k)}{n}}\\\\&amp;amp;=\\dfrac{(\\lambda t)^{k} e^{-\\lambda t}}{k!}\\\\&amp;amp;\\Sigma_{i=1}^{N}k_{i}\\rho-\\theta^{2};\\phi,\\Phi\\end{align*}\\)这就是参数为 \\(\\lambda\\, t\\) 的Poisson过程，其强度 \\(\\lambda\\)，尽管这种证明是不严格的，但是它揭示了不同分布在样本足够大的情况下有着渐进统一的关系。A strict proof of Poisson process要证明在t时刻事件出现n次的概率是泊松分布，因为有平稳独立增量可以构造:\\[\\begin{array}{l} \\begin{align*}P\\left\\lbrace \\left[ N(t)-N(0)\\right] =n\\right\\rbrace = P\\left\\lbrace\\left[ N(t+t_{0}-N(t))\\right]= n \\right\\rbrace &amp;amp;= \\dfrac{(\\lambda t)^{k} e^{-\\lambda t}}{k!},(n=0,1,2,\\dots) \\end{align*}\\end{array}\\]First we consider the initial condition,\\[\\begin{array}{l} \\begin{align*}P_{0}(t+t_{0})&amp;amp;=P\\left\\lbrace N(t+t_{0}=0)\\right\\rbrace \\\\ &amp;amp;= P\\left\\lbrace N(t)=0,N(t+h)-N(t)=0 \\right\\rbrace \\\\&amp;amp;= P_{0}(t)\\left[ 1-\\lambda t_{0} +o(t_{0})\\right] \\end{align*} \\end{array}\\]\\[\\Longrightarrow \\dfrac{P_{0}(t+t_{0})-P_{0}(t)}{t_{0}}=-\\lambda P_{0}(t)+\\dfrac{o(t_{0})}{t_{0}}\\]When \\(t_{0}\\longrightarrow 0\\) , we have \\(\\dfrac{dP_{0}(t)}{dt}=-P_{0}(t)\\lambda\\) and \\(P_{0}(0)=1\\) Therefore, \\(P_{0}(t)=e^{-\\lambda t}\\) Then,\\[\\begin{align*}P_{n}(t+t_{0})&amp;amp;=P_{n}(t)P_{0}(h)+P_{n-1}(t)P_{1}(h)+o(h)\\\\ \\\\{\\lim_{t_{0} \\to 0}}\\dfrac{P_{n}(t+t_{0})-P_{n}(t)}{t_{0}}&amp;amp;={\\lim_{t_{0} \\to 0}}-\\lambda P_{n}(t)+\\lambda P_{n-1}(t)+ \\dfrac{o(t_{0})}{t_{0}}\\\\\\dfrac{dP_{0}(t)}{dt}&amp;amp;=-\\lambda P_{n}(t)+\\lambda P_{n-1}(t)\\end{align*}\\]We will have: \\(\\dfrac{d\\left[ e^{\\lambda t}P_{n}(t)\\right]}{dt}=\\lambda e^{\\lambda t}P_{n-1}(t)\\)Since \\(P_{1}(t)=\\lambda t e^{-\\lambda t}\\)We can find that: \\(\\lambda e^{\\lambda t}P_{n-1}(t)=\\dfrac{\\lambda (\\lambda t)^{n-1}}{(n-1)!}\\)With the initial condition,\\[P_{n}(t)=\\dfrac{(\\lambda t)^{k} e^{-\\lambda t}}{k!}\\]Given the second definition, it is a poisson process.泊松过程的数字特征我们已经知道了 \\({X(t),t\\geq0}\\) 是一个Poisson过程，那么 \\(X(t)=\\dfrac{(\\lambda t )^ke^{-\\lambda t}}{k!}\\) (密度函数) 且之前泊松过程特征函数那一章有: \\(m_{X}(t)=E(X_{t})=\\lambda t\\)我们可以推导出:\\[\\begin{align*} D_{X}(t) =Var(X(t)) &amp;amp;= E(X^{2}(t))-E^{2}(N(t))\\\\ &amp;amp;=\\sum_{n=0}^{\\infty}\\dfrac{n^{2}(\\lambda t)^{n} e^{-\\lambda t}}{n!}-(\\lambda t)^{2}\\\\ &amp;amp;=\\lambda t e^{-\\lambda t} \\sum_{n=1}^{\\infty}\\dfrac{(n-1+1)(\\lambda t)^{n-1} }{n-1!}-(\\lambda t)^{2}\\\\ &amp;amp;=\\lambda t e^{-\\lambda t} \\left[ \\sum_{n=2}^{\\infty}\\dfrac{(\\lambda t)(\\lambda t)^{n-2} }{n-2!}+\\sum_{n=1}^{\\infty}\\dfrac{(\\lambda t)^{n-1}}{(n-1)!}\\right] -(\\lambda t)^{2}\\\\ &amp;amp;=\\lambda t e^{-\\lambda t} \\left[ \\lambda t e^{\\lambda t}+e^{\\lambda t}\\right] -(\\lambda t)^{2}\\\\ &amp;amp;= \\lambda t\\end{align*}\\]发现泊松过程的方差仍为 \\(\\lambda t\\) 自相关函数 \\(R_{X}(s,t)=E(X(s)X(t)) (0&amp;lt;s&amp;lt;t)\\) , 协方差函数 \\(\\gamma_{X}(s,t)=R_{X}(s,t)-m_{X}(s)m_{X}(s)\\) 因为泊松过程有独立平稳增量，可以改写\\[\\begin{align*} E(X(s)X(t))&amp;amp;=E(X(s)[X(t)-X(s)+X(s)])\\\\&amp;amp;=E(X(s)[X(t)-X(s)])+E(X^{2}(s))\\\\&amp;amp;=\\lambda s \\lambda (t-s)+(\\lambda s)^{2}+\\lambda s\\\\&amp;amp;=\\lambda^{2}st+\\lambda s \\\\ \\gamma_{X}(s,t)&amp;amp;=\\lambda^{2}st+\\lambda s-\\lambda^{2}st\\\\&amp;amp;= \\lambda min(s,t)\\end{align*}\\]泊松过程相关关于之前的等待时间，给出结论: \\(T_{n}\\) 是更新的时间间隔，也就是两次事件发生的间隔，\\(T_{n}\\) 会服从 \\(P\\left\\lbrace T_{n}\\leq t\\right\\rbrace =1-e^{-\\lambda t}\\) 的指数分布函数\\(W_{n}\\) 则是第n次时间发生/到达的时间，\\(W_{n}\\) 是[0,t]上均匀分布的顺序统计量，其联合分布可以写成:Given \\(N(t)=n\\) , we have \\(f(T_{1},T_{2}, \\dots,T_{n})=\\dfrac{n!}{t^{n}}\\)对于复合泊松过程，如 \\(X(t)=\\sum_{i=1}^{N(t)}\\xi_{i},\\xi_{i}\\) 均独立同分布，根据特征函数可以求得：\\[\\begin{array}{l}\\begin{align*}E[e^{iu\\sum_{n=1}^{k}\\xi_{n}}]&amp;amp;=\\prod_{n=1}^{k}E(e^{iu\\xi_{n}})=\\varphi_{\\xi}^{k}(u) \\\\\\Rightarrow\\varphi_{X(t)}(u)&amp;amp;=E[\\varphi_{\\xi}^{N(t)}(u)] \\end{align*}\\end{array}\\]Finally we will get: \\(\\begin{align*}E[X(t)]&amp;amp;=\\lambda t E(\\xi)\\\\D(X(t))&amp;amp;=\\lambda t E(\\xi^{2})\\end{align*}\\)[link] Python模拟泊松过程。几道经典的例题医院从早上8点开始接诊，每次专家只能看一名患者，平均需要20分钟，每名患者花的时间都是独立同指数分布。现求早上8点到12点门诊结束，成功就诊患者总共的等待时间。(1)每人花费时间平均20mins \\(\\Rightarrow\\lambda=3 persons/hour\\)(2)问题中有两个随机变量，就诊时间(服从指数分布)和就诊人数(泊松过程)，可以先固定一个量 Solution:\\[E[\\sum_{n=1}^{N(4)}T_{n}]=E\\left\\lbrace E\\left[ \\sum_{n=1}^{N(4)}T_{n}\\arrowvert N(4)\\right] \\right\\rbrace\\]Suppose N(4)=k, then, \\\\[E\\left[ \\sum_{n=1}^{k}T_{n}\\arrowvert N(4)=k\\right] = \\sum_{n=1}^{k}E\\left[T_{n}\\right] =k \\dfrac{4}{2}\\]\\(T_{n}\\) should obey a uniform Distr. for T in[0,4]\\[\\Rightarrow 2 E[N(4)] = 24\\]平均每6分钟一名顾客进商场，这一现象可以视作服从泊松过程。顾客进入商场购物的概率是0.6，每位顾客是否购买相互独立，且不受进入商城人数影响，求商场从早上10点营业开始到22点关门，单次购买商品顾客的人数。 tip: 复合泊松过程,进入商城人数的泊松过程X(t), \\(\\lambda=10\\) persons/hour ,购买人数期望值 \\(E(\\xi)=0.6\\)Solution:\\[\\begin{align*} E(N(12))&amp;amp;=E[\\sum_{n=1}^{X(t)}\\xi_{n}]\\\\ &amp;amp;=\\lambda t E(\\xi)\\\\ &amp;amp;= 10 * 12 * 0.6=720 \\end{align*}\\][0,t]时间内某系统受到冲击的次数N(t)形成参数为 \\(\\lambda\\) 的Possion过程。每次冲击会造成 \\(Y_{i}(i=1,2,3,\\dots,n)\\) 独立同分布的指数分布，其均值为 \\(\\mu\\) 。设累计损害超过A时，系统就会终止运行。以T记系统运行时间/寿命，求系统平均寿命E(T)。(对于非负随机变量 \\(E(T)=\\int_{0}^{\\infty}P\\left\\lbrace T&amp;gt;t\\right\\rbrace dt\\) )tip:这里需要引进另一个分布 \\(\\varGamma(n,\\lambda)\\) 的概率密度函数 \\(f(t)=\\dfrac{\\lambda e^{-\\lambda t}(\\lambda t)^{n-1}}{\\varGamma(n)}\\) 当n=1时，\\(\\varGamma(1,\\lambda)\\) 退化为指数分布,\\(\\varGamma(n)=(n-1)!\\) Another tip:求积分的时候会用到 \\(\\int_{0}^{\\infty}\\lambda e^{-\\lambda t}(\\lambda t)^{n}dt=\\varGamma(n+1)=n!\\)Text finished in 1st May, 2020" } ]
