<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Text Multilabel Classification using BERT" /><meta property="og:locale" content="en" /><meta name="description" content="This is a course project, applying BERT in text classification. The project will go through the entire process of the task." /><meta property="og:description" content="This is a course project, applying BERT in text classification. The project will go through the entire process of the task." /><link rel="canonical" href="https://goodeda.github.io/posts/Text-classification-with-BERT/" /><meta property="og:url" content="https://goodeda.github.io/posts/Text-classification-with-BERT/" /><meta property="og:site_name" content="Zhixu" /><meta property="og:image" content="https://goodeda.github.io/assets/post_img/Distl_acc.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-01T09:00:00+03:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://goodeda.github.io/assets/post_img/Distl_acc.png" /><meta property="twitter:title" content="Text Multilabel Classification using BERT" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-13T23:43:50+03:00","datePublished":"2022-05-01T09:00:00+03:00","description":"This is a course project, applying BERT in text classification. The project will go through the entire process of the task.","headline":"Text Multilabel Classification using BERT","image":"https://goodeda.github.io/assets/post_img/Distl_acc.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://goodeda.github.io/posts/Text-classification-with-BERT/"},"url":"https://goodeda.github.io/posts/Text-classification-with-BERT/"}</script><title>Text Multilabel Classification using BERT | Zhixu</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Zhixu"><meta name="application-name" content="Zhixu"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/favicon/android-chrome-512x512.png " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Zhixu</a></div><div class="site-subtitle font-italic">NLP, mathematics and languages.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fa fa-cubes ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-user-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/goodeda" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['guzhixu','outlook.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Text Multilabel Classification using BERT</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Text Multilabel Classification using BERT</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/username">zhixu</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1651384800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-05-01 </em> </span> <span> Updated <em class="timeago" data-ts="1660423430" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-08-13 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1390 words"> <em>7 min</em> read</span></div></div></div><div class="post-content"><h3 id="project-introduction"><span class="mr-2">Project introduction</span><a href="#project-introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>In this project, the goal is to do multilabel classification. A piece of news can be related to only one topic or several. The task is training a model to classify relevant themes of news. Therefore, this is also a <strong>multilabel classification</strong> problem.<br /> I did the work of data exploring, cleaning and turning them into batches. Then I used RoBERTa and distilled BERT to tackle the task, the latter of which is lighter but has a similar performance as original model. I also compared two models and validate them on development dataset.</p><h3 id="text-extraction--data-exploration"><span class="mr-2">Text extraction &amp; data exploration</span><a href="#text-extraction--data-exploration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>The content of news as well as its label is embedded in xml files. So after downloading the file, the first thing we should do is extracting useful information from files.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">read_file</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="nb">file</span><span class="o">=</span><span class="n">etree</span><span class="p">.</span><span class="n">parse</span><span class="p">(</span><span class="s">'./%s'</span><span class="o">%</span><span class="n">file_path</span><span class="p">)</span><span class="c1"># replace it with file_path
</span>    <span class="k">return</span> <span class="nb">file</span><span class="p">.</span><span class="n">getroot</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">extract_text</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="n">root</span><span class="o">=</span><span class="n">read_file</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="n">file_id</span><span class="o">=</span><span class="n">root</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"/newsitem/@itemid"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span><span class="o">=</span><span class="p">{}</span>
    <span class="k">if</span> <span class="n">root</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"//headline/text()"</span><span class="p">)</span><span class="o">!=</span><span class="p">[]:</span>
        <span class="n">data</span><span class="p">[</span><span class="n">file_id</span><span class="p">][</span><span class="s">"HEADLINE"</span><span class="p">]</span><span class="o">=</span><span class="n">root</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"//headline/text()"</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">capitalize</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data</span><span class="p">[</span><span class="n">file_id</span><span class="p">][</span><span class="s">"HEADLINE"</span><span class="p">]</span><span class="o">=</span><span class="s">''</span>
    <span class="n">data</span><span class="p">[</span><span class="n">file_id</span><span class="p">][</span><span class="s">"TEXT"</span><span class="p">]</span><span class="o">=</span><span class="n">txt_clean</span><span class="p">(</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"//text/p/text()"</span><span class="p">)))</span>
    <span class="c1"># data[file_id]["TEXT"]=' '.join(root.xpath("//text/p/text()")).strip()
</span>    <span class="n">data</span><span class="p">[</span><span class="n">file_id</span><span class="p">][</span><span class="s">"LABEL"</span><span class="p">]</span><span class="o">=</span><span class="n">root</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">"//codes[@class='bip:topics:1.0']/code/@code"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">directory</span> <span class="ow">in</span> <span class="n">filedir</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"%s directory completed"</span><span class="o">%</span><span class="n">directory</span><span class="p">)</span>
    <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="s">'../src-data/%s'</span><span class="o">%</span><span class="n">directory</span><span class="p">):</span>
        <span class="n">extract_text</span><span class="p">(</span><span class="s">'../src-data/%s/%s'</span><span class="o">%</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="nb">file</span><span class="p">))</span>
</pre></table></code></div></div><p>I use lxml.etree parsing the format and xpath syntax to obtain contents and labels. Then do some data explorations. I sort of want to see how different labels are distributed; if they are unevenly labeled? What’s the most frequent label in thousands of texts.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">df_lb</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">code_label</span><span class="p">)</span>
<span class="n">df_lb</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"../data_files/label-codes.csv"</span><span class="p">)</span>
<span class="n">array_label</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"label"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># this corresponds to the 3rd figure in report, 10 most frequent labels
</span><span class="n">topN</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">freq_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">array_label</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">topN</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">array_label</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="n">topN</span><span class="p">:])</span>
<span class="n">mostfreq_classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">code_label</span><span class="p">[</span><span class="s">"code"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">freq_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">leastfreq_classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">code_label</span><span class="p">[</span><span class="s">"code"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">freq_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">most_frequency</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">array_label</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">topN</span><span class="p">]</span>
<span class="n">least_frequency</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">array_label</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="o">-</span><span class="n">topN</span><span class="p">:]</span>
<span class="n">most_label</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"most_classes"</span><span class="p">:</span><span class="n">mostfreq_classes</span><span class="p">,</span><span class="s">"frequency"</span><span class="p">:</span><span class="n">most_frequency</span><span class="p">})</span>
<span class="n">least_label</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"least_classes"</span><span class="p">:</span><span class="n">leastfreq_classes</span><span class="p">,</span><span class="s">"frequencies"</span><span class="p">:</span><span class="n">least_frequency</span><span class="p">})</span>
<span class="n">label_data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">most_label</span><span class="p">,</span><span class="n">least_label</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p>The frequency of labels turns out to be pretty imbalanced. Take a look at the following picture, we can see the most frequent label <strong>CCAT</strong>.<br /> <img data-src="https://raw.githubusercontent.com/goodeda/goodeda.github.io/main/assets/post_img/Top10-frequent-labels.png" alt="" data-proofer-ignore> And if we compare the top 10 most frequent labels and 10 least frequent ones, we see there is a really large gap. So this issue could lead the model to focus more on the frequent labels instead of rare tags.<br /> Another discovery is that there are totally 126 different topics. Thereby the output of the neural model should be a vector including 126 elements standing for possiblities of the topics.<br /> How about the texts? The maximum tokens BERT model can deal with as input is 512. So if text length exceeds the limitation too much, only part of contents will be processed.<br /> <img data-src="https://raw.githubusercontent.com/goodeda/goodeda.github.io/main/assets/post_img/text-length-distribution.png" alt="" data-proofer-ignore> OK, we can see majorities are within 512 tokens.</p><h3 id="data-transformation-and-preparation"><span class="mr-2">Data transformation and preparation</span><a href="#data-transformation-and-preparation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ol><li>Since the label is in format of string. For example, [‘CCAT’, C15’’] or [‘M14’]. In order to convert the labels into a format we can use for calculating loss, it should be in one-hot encoding.</ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1">#transforming labels into one-hot encoding
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>
<span class="n">mlb</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">df_lb</span><span class="p">[</span><span class="s">"code"</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">labels2binaries</span><span class="p">(</span><span class="n">label</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="n">mlb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="n">onehot_label</span> <span class="o">=</span> <span class="n">labels2binaries</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">"LABEL"</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span> <span class="o">=</span> <span class="n">onehot_label</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">df</span><span class="p">[[</span><span class="s">'text'</span><span class="p">,</span><span class="s">'label'</span><span class="p">]].</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"../data_files/lower_nosep_data.csv"</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></table></code></div></div><ol><li>Preparing data The texts are still strings while machine doesn’t recognize. For machine, each word is represented by an id. What’s more, we usually use mini-batch for training in practice, allowing the process running in parallel. With the help of Dataset and pre-trained tokenizer, it’s not hard to achieve those goals.</ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">RobertaTokenizer</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">Tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'roberta-base'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">allinfo</span> <span class="o">=</span> <span class="n">Data</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">allinfo</span><span class="p">[</span><span class="s">'text'</span><span class="p">]</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">allinfo</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">max_len</span> <span class="o">=</span> <span class="n">max_len</span>
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode_plus</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">text</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
    <span class="bp">None</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">max_len</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s">'max_length'</span><span class="p">,</span>
    <span class="n">return_token_type_ids</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s">'attention_mask'</span><span class="p">]</span>
    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s">"token_type_ids"</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="s">'ids'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span><span class="s">'mask'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span>
            <span class="s">'token_type_ids'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span>
            <span class="s">'targets'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)}</span>
</pre></table></code></div></div><p>Here CustomDataset inherit from its parent class–Dataset. We, however, need to rewrite some methods. There is one thing worth to mention: in encode_plus(), parameter add_special_tokens=True, meaning every time at the beginning of a sentence, there will be a special token [CLS] and at the end [SEP]; padding=True means those texts whose tokens are less than max tokens will be filled with special tokens. Thereby sentence length is always the same. truncation=True is to make sure the sentence length won’t beyond the limitation.</p><p>There might be one more confusing thing about the code–What is mask? Mask is to prevent the model from seeing the true id and let it guess what’s the true word. This is implemented in pre-training round of BERT.</p><h3 id="building-model"><span class="mr-2">Building model</span><a href="#building-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>First block is BERT, and feed the model with input id list.<br /> And then pass the sentence embedding into a feed-forward network and predict corresponding labels.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">BERTClass</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BERTClass</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">DistilBertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'distilbert-base-uncased'</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">768</span><span class="p">),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">126</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">output_1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">l1</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">output_1</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:].</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BERTClass</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="training"><span class="mr-2">Training</span><a href="#training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>Then we apply the model in training. The step consists of getting batch data, training &amp; saving the model and finally evaluating the accuracy.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">fin_targets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fin_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Data_loading</span><span class="p">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'ids'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="c1"># print(torch.max(ids),torch.min(ids))
</span>        <span class="c1"># print(torch.max(mask),torch.min(mask))
</span>        <span class="c1"># token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
</span>        <span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'targets'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">fin_targets</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">targets</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">().</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">fin_outputs</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">outputs</span><span class="p">).</span><span class="n">cpu</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">().</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">_</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: {}, batch:{}, Avg Loss:  {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">total_loss</span><span class="o">/</span><span class="p">(</span><span class="n">_</span><span class="o">+</span><span class="mi">1</span><span class="p">)),</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'epoch'</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s">'state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s">'optimizer'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()}</span>
    <span class="k">return</span> <span class="n">fin_targets</span><span class="p">,</span> <span class="n">fin_outputs</span><span class="p">,</span> <span class="n">total_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">Data_loading</span><span class="p">.</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">checkpoint</span>
</pre></table></code></div></div><p>The training process takes quite a long time. For RoBERTa model, about 20+ hours is spent. As expected, the distilled model has a faster training step, with about 12 hours.<br /> The validation step basically remains the same but doesn’t save models.</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre>#!/bin/bash
#SBATCH -n 1 # node you request
#SBATCH -p gpu # use gpu
#SBATCH -t 20:00:00 # time for gpu
#SBATCH --mem= Memory size (GB)
#SBATCH --gres=gpu:v100:1 
#SBATCH -J name of the project
#SBATCH -o &lt;outcome file path&gt;
#SBATCH -e &lt;error file path&gt;
#SBATCH --account= &lt;your project id&gt;
#SBATCH --mail-type=ALL
#SBATCH --mail-user= &lt;your email address&gt;

module purge
module load pytorch
python train.py
</pre></table></code></div></div><p>At the university, I use <em><a href="https://research.csc.fi/-/puhti">Puhti</a></em> to enable GPU training and above show some bash commands and configurations.</p><h3 id="evaluation"><span class="mr-2">Evaluation</span><a href="#evaluation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>The following plot is the performance of distlBERT. <img data-src="https://raw.githubusercontent.com/goodeda/goodeda.github.io/main/assets/post_img/Distl_acc.png" alt="" data-proofer-ignore> After several epochs, the model reaches about 70% in accuracy.<br /> Here the way of calculating accuracy is somewhat different from usual one. The model could miss one true topic or predict a wrong topic. The accuracy computation is achieved via <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">sklearn.metrics.accuracy_score</a>.<br /> Only if the predicted labels match exactly the true values, will it be counted as a correct prediction. There are quite a few types of topics, so each topic receives an accuracy value. The final one is the averaged value. In fact, I also applied f1 score (both micro and macro).<br /> The results show that RoBERTa performs better than DistlBERT. Although, the compressed model falls behind 1~2% in accuracy, it indeed saves more time.</p><h3 id="project-review"><span class="mr-2">Project review</span><a href="#project-review" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>Overall in this project, I go through the pipeline of solving a NLP problem.<br /> I familiarized with the necessary steps of text classification. Also, I had hands-on experience of using BERT model.<br /> There are actually more things can be done to improve the results. For example, text cleaning or how to deal with the imbalanced labels. Efforts of data augmentation should be made as well. <br /> Some codes are adopted from the internet. The followings are some sites I referred while doing the project.</p><p><a href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8">BERT, RoBERTa, DistilBERT, XLNet — which one to use?</a><br /> <a href="https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1">Transformers for Multi-Label Classification made simple</a><br /> <a href="https://kyawkhaung.medium.com/multi-label-text-classification-with-bert-using-pytorch-47011a7313b9">Multi-label Text Classification with BERT using Pytorch</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/english/'>English</a>, <a href='/categories/projects/'>Projects</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/projects/" class="post-tag no-text-decoration" >Projects</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Text Multilabel Classification using BERT - Zhixu&amp;url=https://goodeda.github.io/posts/Text-classification-with-BERT/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Text Multilabel Classification using BERT - Zhixu&amp;u=https://goodeda.github.io/posts/Text-classification-with-BERT/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://goodeda.github.io/posts/Text-classification-with-BERT/&amp;text=Text Multilabel Classification using BERT - Zhixu" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/SVD-PCA/">PCA & SVD</a><li><a href="/posts/app-visual/">App analysis with review information on google play</a><li><a href="/posts/Power-BI-Practice/">Power BI practice for data visualization</a><li><a href="/posts/Logistic-Regression/">Logistic Regression</a><li><a href="/posts/bisystem/">Virtual machine on Windows</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/p%C3%A4iv%C3%A4kirja/">päiväkirja</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/projects/">Projects</a> <a class="post-tag" href="/tags/data-structure-algorithm/">data-structure-algorithm</a> <a class="post-tag" href="/tags/language-learning/">Language-learning</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/power-bi/">POWER-BI</a> <a class="post-tag" href="/tags/visualization/">Visualization,</a> <a class="post-tag" href="/tags/word-segmentation/">word-segmentation</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Hackathon22/"><div class="card-body"> <em class="timeago small" data-ts="1653939720" > 2022-05-30 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DHH22 Hackathon Project</h3><div class="text-muted small"><p> Project Background The project is a topic of Helsinki Digital Humanities Hackathon 2022.The theme is Network of Parlamint. The data include national parliament speeches of 17 european countries. We...</p></div></div></a></div><div class="card"> <a href="/posts/Prediction-of-client-loss/"><div class="card-body"> <em class="timeago small" data-ts="1660888800" > 2022-08-19 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Analyse the loss of airline client</h3><div class="text-muted small"><p> Project description So this project was acutally done in my undergraduate course–machine learning &amp;amp; data mining. The topic is about the airline company’s customer analysis and prediction. The d...</p></div></div></a></div><div class="card"> <a href="/posts/SSH-KEY/"><div class="card-body"> <em class="timeago small" data-ts="1661369400" > 2022-08-24 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Password-less for connecting Github and remote server</h3><div class="text-muted small"><p> I used to enter the password when pushing file to Github or connecting to remote server. I always feel this step redundant and recently came to konw that SSH is a good solution. I want to record th...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Matkani-Ruotsissa/" class="btn btn-outline-primary" prompt="Older"><p>Matkani Ruotsissa</p></a> <a href="/posts/Graphs-shortst-path/" class="btn btn-outline-primary" prompt="Newer"><p>Graph and shortest path</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "goodeda/goodeda.github.io", "data-repo-id": "", "data-category": "", "data-category-id": "", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/username">zhixu</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/p%C3%A4iv%C3%A4kirja/">päiväkirja</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/projects/">Projects</a> <a class="post-tag" href="/tags/data-structure-algorithm/">data-structure-algorithm</a> <a class="post-tag" href="/tags/language-learning/">Language-learning</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/power-bi/">POWER-BI</a> <a class="post-tag" href="/tags/visualization/">Visualization,</a> <a class="post-tag" href="/tags/word-segmentation/">word-segmentation</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
